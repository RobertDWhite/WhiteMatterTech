[{"content":" Introduction Logging and traffic monitoring are of utmost importance in information security. Having searchable stored logs can allow visibility into a variety of critical activities related to a data breach.\nFor example, individual computer event logs can provide insight into an attacker\u0026rsquo;s lateral movement within an environment. Active Directory authentication logs can provide more detail into this lateral movement and even help to establish a timeline of this movement. Firewall logs can provide insight into an attacker\u0026rsquo;s first contact or the first time an attacker utilized a particular command or control domain. NetFlow logs allow visibility into how a user interacts with other devices internally.\nWhile there are many more examples of logging techniques providing critical data, especially in the event of a breach, the aforementioned logging methods highlight the importance of logging and retaining these data.\nMy goal is not to provide a thesis concerning logging and monitoring. I highly recommend taking a closer look at some of the theoretical and pragmatic reasons for logging in a systems security environment.\n Graylog Graylog provides a powerful open-source solution for log management that perfectly suits our needs for our homelab environment and beyond. I certainly do not want to diminish how powerful and capable Graylog is. Graylog is more than suitable for enterprise environments and has more enterprise-friendly offerings available as well. For our needs running on Unraid, however, the open-source version will be more than sufficient.\nGraylog has been notoriously difficult to get running successfully on Unraid in the past. Docker Compose eliminates many of the issues that complicated the install in the past, and therefore, we will be using Docker Compose on Unraid to alleviate these issues and get a working Graylog instance.\n Prepping Unraid In order to use this particular setup, you will first need to install Docker Compose Manager. You can install Docker Compose Manager on Unraid from the Community Applications page. Simply search \u0026ldquo;Docker Compose Manager\u0026rdquo; and install the package from \u0026ldquo;dcflachs\u0026rdquo;. Check out the forum post for Docker Compose here. See the image below.\nNote: At the time of this writing, Unraid\u0026rsquo;s Docker Compose implementation is in beta, but I have been using it since it first launched. I have NEVER had any issues on my Unraid system due to Docker Compose. Nonetheless, it is a good idea to always have backups and parity in place before dealing with beta products specifically. Once installed, we can move to the fun part.\n Configuring a Docker Compose Stack on Unraid This step will require that you have Docker enabled on your Unraid host. If you have not yet done this, I highly recommend checking out Unraid\u0026rsquo;s Wiki for more information about how Docker is implemented in Unraid and how to enable it on your system.\nOnce enabled, go to \u0026ldquo;Docker\u0026rdquo; in your menu. Scroll to the bottom of the page, and you should see a \u0026ldquo;Compose\u0026rdquo; section if your install went correctly above.\nSelect ADD NEW STACK and name the stack something obvious like \u0026ldquo;graylog\u0026rdquo;, like shown in the image below. Next, click the gear icon next to your \u0026ldquo;graylog\u0026rdquo; stack, and select EDIT STACK. I new large text edit box should appear, and you will see something like \u0026ldquo;Editing /boot/config/plugins/compose.manager/projects/graylog/compose.yml\u0026rdquo;. Copy and paste the code below into the box:\nversion: \u0026#34;3.8\u0026#34;\rservices:\rmongodb:\rimage: \u0026#34;mongo:5.0\u0026#34;\rvolumes:\r- \u0026#34;/your/unraid/path/here:/data/db\u0026#34;\rrestart: \u0026#34;on-failure\u0026#34;\relasticsearch:\renvironment:\rES_JAVA_OPTS: \u0026#34;-Xms1g -Xmx1g -Dlog4j2.formatMsgNoLookups=true\u0026#34;\rbootstrap.memory_lock: \u0026#34;true\u0026#34;\rdiscovery.type: \u0026#34;single-node\u0026#34;\rhttp.host: \u0026#34;0.0.0.0\u0026#34;\raction.auto_create_index: \u0026#34;false\u0026#34;\rimage: \u0026#34;docker.elastic.co/elasticsearch/elasticsearch-oss:7.10.2\u0026#34;\rulimits:\rmemlock:\rhard: -1\rsoft: -1\rvolumes:\r- \u0026#34;/your/unraid/path/here:/usr/share/elasticsearch/data\u0026#34;\rrestart: \u0026#34;on-failure\u0026#34;\rgraylog:\rimage: \u0026#34;graylog/graylog:4.2\u0026#34;\rdepends_on:\relasticsearch:\rcondition: \u0026#34;service_started\u0026#34;\rmongodb:\rcondition: \u0026#34;service_started\u0026#34;\rentrypoint: \u0026#34;/usr/bin/tini -- wait-for-it elasticsearch:9200 -- /docker-entrypoint.sh\u0026#34;\renvironment:\rGRAYLOG_TIMEZONE: \u0026#34;America/New_York\u0026#34;\rTZ: \u0026#34;America/New_York\u0026#34;\rGRAYLOG_NODE_ID_FILE: \u0026#34;/usr/share/graylog/data/config/node-id\u0026#34;\rGRAYLOG_PASSWORD_SECRET: \u0026#34;enter secret here\u0026#34;\rGRAYLOG_ROOT_PASSWORD_SHA2: \u0026#34;enter SHA2 of secret here\u0026#34;\rGRAYLOG_HTTP_BIND_ADDRESS: \u0026#34;0.0.0.0:9000\u0026#34;\rGRAYLOG_HTTP_EXTERNAL_URI: \u0026#34;http://localhost:9000/\u0026#34;\rGRAYLOG_ELASTICSEARCH_HOSTS: \u0026#34;http://elasticsearch:9200\u0026#34;\rGRAYLOG_MONGODB_URI: \u0026#34;mongodb://mongodb:27017/graylog\u0026#34;\rports:\r- \u0026#34;5044:5044/tcp\u0026#34; # Beats\r- \u0026#34;5140:5140/udp\u0026#34; # Syslog\r- \u0026#34;5140:5140/tcp\u0026#34; # Syslog\r- \u0026#34;5555:5555/tcp\u0026#34; # RAW TCP\r- \u0026#34;5555:5555/udp\u0026#34; # RAW TCP\r- \u0026#34;9000:9000/tcp\u0026#34; # Server API\r- \u0026#34;12201:12201/tcp\u0026#34; # GELF TCP\r- \u0026#34;12201:12201/udp\u0026#34; # GELF UDP\r- \u0026#34;10000:10000/tcp\u0026#34; # Custom TCP port\r- \u0026#34;10000:10000/udp\u0026#34; # Custom UDP port\r- \u0026#34;13301:13301/tcp\u0026#34; # Forwarder data\r- \u0026#34;13302:13302/tcp\u0026#34; # Forwarder config\rvolumes:\r- \u0026#34;/your/unraid/path/here:/usr/share/graylog/data/data\u0026#34;\r- \u0026#34;/your/unraid/path/here:/usr/share/graylog/data/journal\u0026#34;\rrestart: \u0026#34;on-failure\u0026#34;\rvolumes:\rmongodb_data:\res_data:\rgraylog_data:\rgraylog_journal:  Your Unraid should look like the image below: Before saving, you will need to edit Lines 7, 23, 59, and 60. Replace the entire /your/unraid/path/here with the full path to where you would like to store Graylog files. For example, I used /mnt/disk7/graylog/mongodb_data for Line 7, /mnt/disk7/graylog/es_data for Line 23, /mnt/disk7/graylog/graylog_data for Line 59, and /mnt/disk7/graylog/graylog_journal for Line 60.\nNotice you do not change or modify the \u0026quot; : \u0026quot; or anything after it. For example, the full Line 7 would look like \u0026quot;/mnt/disk7/graylog/mongodb_data:/data/db\u0026quot; in my example.\nIn my specific use case, I wanted Graylog to be stored on a particular disk outside of my cache drive for longer term storage. Additionally, I did not want to eat up Docker image storage or cache storage in general, and putting all the Graylog logs on a another disk made the most sense.\nImportant Note: DO NOT put all of the folders in the same subdirectory. You can see I used /mnt/disk7/graylog as the root of the Graylog data, but each volume in the Docker Compose has its own subdirectory. Putting ALL of the Graylog volumes in the same subdirectory may have unintended detrimental consequences. I am not even sure it will run. Nonetheless, avoid it.\nFinally, before saving, you must create a password and a SHA2 hash of the password. These values go on Lines 38 and 39 respectively. For this, I actually used another Docker container on Unraid called CyberChef (mpepping/cyberchef). You also can use terminal on Unraid. Click the Terminal icon on Unraid in the top right of the menu. Copy and Paste the following:\necho -n \u0026#34;Enter Password: \u0026#34; \u0026amp;\u0026amp; head -1 \u0026lt;/dev/stdin | tr -d \u0026#39;\\n\u0026#39; | sha256sum | cut -d\u0026#34; \u0026#34; -f1 Click Enter, and you will be prompted to enter the password you will use for Graylog (which should go on Line 38). One you enter your password, click Enter again, and Terminal should output the required SHA2 hash of your password. Copy that hash and paste it in Line 39 of the docker-compose.yml on Unraid. See the image below for an example of this step. FINALLY, click SAVE CHANGES at the top.\n Start the Stack - Compose Up Now, you can click COMPOSE UP under Commands associated with your graylog stack. This is the equivalent of the traditional Docker Compose command:\ndocker-compose up -d You should now see a new popup called \u0026ldquo;Stack graylog Up\u0026rdquo;, and you should observe Docker Compose looking for and pulling missing images. Once all the required files are obtained, the final ouput should be similar to the following:\nContainer graylog-mongodb-1 Running\rContainer graylog-elasticsearch-1 Running\rContainer graylog-graylog-1 Running If you see this, you have successfully launched your Graylog containers including the required Docker networking and dependencies for your Unraid host.\nYou should now see your three containers at the top of your running containers list on Unraid in the Docker menu. See the image below for an example: You can now go to your Unraid\u0026rsquo;s IP at port 9000 to access your Graylog instance (e.g., 172.16.1.10:9000). Login with the username admin and the yourpassword you added to Line 38 (e.g., NOT your SHA2 hash).  Bonus: Send Other Unraid Container Syslogs to Graylog The first step required to send syslog to Graylog is to enable Syslog TCP incoming connections on Graylog. To do this, select System / Inputs on the Graylog interface, then choose Inputs. In the Select input dropdown box, scroll to Syslog TCP. Click Launch new input.\nGive your input a nice title, and change the incoming port from 514 to 5140. You can leave everything else as default unless you know what you are doing. Scroll to the bottom and select Save. Now, back on Unraid, go to a running container and edit it or add a new container for whatever you would like.\nUnder \u0026ldquo;Extra Parameters\u0026rdquo;, customize the code below and paste it into the Extra Parameters box after any other parameters that are already there.\n--log-driver=syslog --log-opt tag=\u0026#34;varken\u0026#34; --log-opt syslog-address=tcp://YOUR_GRAYLOG_IP:5140 Change varken to be the descriptive name of the container you are using. Change YOUR_GRAYLOG_IP to the IP you used to access your Graylog instance.\nScroll to the bottom of the container config and click SAVE. If all goes well and your network is allowing the communications to succeed, your container should be up, and the syslog for that container should be forwarding to your Graylog instance! Wrapping Up I hope this post was helpful getting Graylog running on Unraid. If you have any issues, questions, or something was not clear in this post, please comment below, email me at robert@whitematter.tech, or make an \u0026ldquo;Issue\u0026rdquo; on GitHub.\n","permalink":"https://whitematter.tech/posts/run-graylog-with-docker-compose-on-unraid/","summary":"Introduction Logging and traffic monitoring are of utmost importance in information security. Having searchable stored logs can allow visibility into a variety of critical activities related to a data breach.\nFor example, individual computer event logs can provide insight into an attacker\u0026rsquo;s lateral movement within an environment. Active Directory authentication logs can provide more detail into this lateral movement and even help to establish a timeline of this movement. Firewall logs can provide insight into an attacker\u0026rsquo;s first contact or the first time an attacker utilized a particular command or control domain.","title":"Run Graylog with Docker Compose on Unraid"},{"content":"Introduction When I initially began posting publicly on this site, my goal was to be able to host my site fully with Docker for containerization. I hadn\u0026rsquo;t experienced any other decent blogging platform besides WordPress at the time, and I was bent on getting WordPress self-hosted with Docker.\nThis goal was achieved, and my first public post details how I used docker-compose to deploy my blog using containers for WordPress and Traefik.\nMy Reasons For Leaving WordPress The WordPress container setup was successful and effective for the better part of a year, but as I gained readers, I also gained an exorbitant amount of spam inundating my WordPress Dashboard. This quickly became an unworkable solution, even with the copious number of spam blocker plugins enabled like Akismet.\nMy reasons for leaving are no different than the rest of the random posts you can find on the issue throughout the Internet. Specifically, I attribute my exodus to the following: Speed, Security, and Spam.\nSpeed The speed difference is mostly due to the differences in the way WordPress processes requests via PHP. Since WordPress is written in PHP, each request queries the WP Database to grab all the associated files with the request to render an HTML file. Only then is this HTML file returned to your browser. In Hugo, a site is built on pre-rendered HTML, which has no comparison in speed. Loading my old site one last time before the full cutover was painfully slow after testing the Hugo rendering.\nSecurity WordPress is generally safe enough, and I never had any caught suspicious traffic beyond the usual bots blocked by my IPS. However, I was informed a few months before my switch that a plugin I was using, which was reputable and developed by a professional team, was susceptible to a cross-site scripting (XSS) attack. Though I promptly removed that plugin, I was still disturbed enough to Google \u0026ldquo;WordPress plugin vulnerabilities.\u0026rdquo;\nSpam This one I already mentioned above, but it was so irritating that I think it warrants its own section. No kidding, I would get upwards of 1000 spam messages a day, even with every reputable spam blocker plugin I could find enabled. I missed some valuable conversations with readers because they were lost in the sea of spam. I will truly not miss the spam.\nConverting from Wordpress Pages \u0026amp; Posts to Markdown The first step requires you to export your WordPress site to XML. This process is straightforward. Check out WordPress\u0026rsquo;s official documentation about how exactly to do this. This will export your entire site. Be sure to select \u0026ldquo;All content\u0026rdquo; in order to include images and posts.\nThe tool I ultimately chose to convert WordPress content to Markdown was the appropriately named wordpress-export-to-markdown. You will need NodeJS and your WordPress export to use this tool. To use wordpress-export-to-markdown, check out the GitHub repo for the project and follow the instructions on the repo.\nInstall Hugo There are many great official instructions on installing Hugo. Check out the Hugo Install Guide. There are instructions for most major OSs, and you can even find instructions for running Hugo in Docker.\nGenerate Site Structure \u0026amp; Choose a Theme Once installed, you will need to have your files ready to go in a working directory. I recommend making a folder \u0026ldquo;hugo\u0026rdquo;. In hugo, create a folder called content. Add your post MD files to hugo/content. Hugo will take your MD files from hugo/content and automatically generate them as posts/blog-posts/etc..\nHere, I recommend going to my GitHub repo for WhiteMatter.tech and looking at how I have structured my Hugo site as an example. Check out themes.gohugo.io for themes, and follow the instructions on the theme on how to setup your config.toml or, as in my case, config.yml. You can find my theme files on GitHub, which are a customized fork of PaperMod.\nRun a Dev Build of Your Site Once your theme, content, and other files are in place, you can run\nhugo server -D This will generate a site accessible by going to http://localhost:1313 in your webbrowser on the same machine from which you ran that command.\nBuild Your Static Public Folder Once you are finished modifying your site and are happy with the outcome, you can finally run the command to build your site\u0026rsquo;s hugo/public folder.\nhugo Yes, that is all! Just run that command and your public folder will be generated.\nServe Your site Nice work! If you have made it this far with a filled hugo/public folder, you are ready to serve your site. There are an unlimited number of ways to do this. A popular method is GitHub Pages, which allows you to host your static site directly from a GitHub repo. I chose to host my site with a local Docker image since I already had the infrastructure in place from my previous WordPress/Traefik setup. Essentially, though, you just need to serve your hugo/public folder. Check out how I did that below.\nOptional: Serve Your Site With Docker Download the docker-compose.yml with curl curl -LJO https://raw.githubusercontent.com/RobertDWhite/hugo-webserver-docker/main/docker-compose.yml Download the docker-compose.yml with wget wget --no-check-certificate --content-disposition https://raw.githubusercontent.com/RobertDWhite/hugo-webserver-docker/main/docker-compose.yml Or you can create your own docker-compose.yml and add the following:\nversion: \u0026#34;3\u0026#34;\rservices:\rnginx:\rimage: nginx:latest\rports:\r- 80:80\rvolumes:\r- /path/to/hugo/public:/usr/share/nginx/html Be sure you change the /path/to/hugo/public to your full path pointing to your hugo/public folder. Once you have your docker-compose.yml, run the following:\ndocker-compose up -d Once you have your port 80 forwarded to you host running Docker, your site should be live to the public!\nIf you choose to go this route, you might consider setting up a reverse proxy with Docker as well. You can see my post on how to accomplish this here: How to Easily Run A Reverse Proxy using Docker\nAdditionally, you should consider taking security measure to harden your network. See more about that below at Self-Hosting Security.\nI like this method the best because I am able to re-run the following:\nhugo to re-build my site any time I make changes that need published, and the Docker container updates in real-time automatically. That way, I do not have to re-configure my server with updated static site files anytime there is a change.\nSelf-Hosting Security If you choose to host your site from within your network, I recommend taking a look at a couple of my older posts. Specifically, check out How to Harden Your Network Security for Your In-Home Web Hosting\nWrapping Up I hope this post was helpful getting WordPress migrated to Hugo. If you have any issues, questions, or something was not clear in this post, email me at robert@whitematter.tech, or make an \u0026ldquo;Issue\u0026rdquo; on GitHub or start a \u0026ldquo;Discussion\u0026rdquo; on GitHub.\nThanks for reading!\n","permalink":"https://whitematter.tech/posts/migrating-from-wordpress-to-hugo/","summary":"Introduction When I initially began posting publicly on this site, my goal was to be able to host my site fully with Docker for containerization. I hadn\u0026rsquo;t experienced any other decent blogging platform besides WordPress at the time, and I was bent on getting WordPress self-hosted with Docker.\nThis goal was achieved, and my first public post details how I used docker-compose to deploy my blog using containers for WordPress and Traefik.","title":"Migrating from Wordpress to Hugo"},{"content":"\t\u0026lt;h1\u0026gt;Privacy Policy for WhiteMatterTech\u0026lt;/h1\u0026gt;  ","permalink":"https://whitematter.tech/privacy-policy/","summary":"\t\u0026lt;h1\u0026gt;Privacy Policy for WhiteMatterTech\u0026lt;/h1\u0026gt;  ","title":"Privacy Policy"},{"content":"","permalink":"https://whitematter.tech/terms-and-conditions/","summary":"","title":"Terms and Conditions"},{"content":"Live Streaming ATEM Mini Pro\n","permalink":"https://whitematter.tech/recommended-products/","summary":"Live Streaming ATEM Mini Pro","title":"Recommended Products"},{"content":"This post will show you how to run Tails OS as a VM with the Persistence feature enabled.\nRunning Tails as a VM is not recommended generally as it defeats many of the security features in Tails. For example, virtualization requires that you trust the hypervisor host, as the hypervisor has extra privileges over a VM that can reduce security and privacy of the VM.\nI recommend reading Tails\u0026rsquo;s official documentation about virtualization considerations before continuing: https://tails.boum.org/doc/advanced_topics/virtualization/\nHowever, there may be many great reasons this VM setup would be beneficial to you.\nI recommend this setup for tasks that call for extra anonymity. One fun reason to run Tails as a VM with Persistence would be to store and access your crypto wallets. You could also use Tails simply to reduce your digital footprint around the web more generally.\nWrite Tails Live to a USB Stick The first step to running Tails on Unraid (with Persistence) is to write the Live OS to a USB stick. You will need a USB stick that can stay attached to your Unraid server anytime you want to access the Tails VM (I leave mine in all the time).\nIf you simply want to run Tails with no Persistence, you can certainly skip this step.\nYou can find documentation about writing the OS to a USB stick from Tails\u0026rsquo;s official site: https://tails.boum.org/install/index.en.html\nI am using this Sony USB stick from Amazon. I am also using this USB stick for my Unraid OS because it is on the recommended USB stick list by the Unraid developers.\nAt the time of the writing of the post, I am using Tails 4.26.\nAttach the USB Stick to Unraid Attach the USB stick to Unraid. Open an SSH connection to Unraid or open the Terminal in the Unraid GUI. Run the command below.\nls -lha /dev/disk/by-id/\nCopy the full location of your USB stick. It should look similar to the one below. Do not copy the ID that contains \u0026ldquo;-PART\u0026rdquo;, etc.\n/dev/disk/by-id/usb-_USB_DISK_3.0_xxxxxxxxxxxxxxxxxx-0:0\n(Note: If you have two or more USB sticks attached to Unraid, and the chances are that you do given Unraid is supposed to run off of a USB stick, you may need to use deductive reasoning or the brand name to differentiate which USB stick you have. If you have trouble identifying the correct USB stick, you could unplug the Tails USB and run the command again in order to isolate your USB.)\nCreate the VM In Unraid\u0026rsquo;s VM manager, select \u0026ldquo;ADD VM.\u0026rdquo; From there, select \u0026ldquo;Linux.\u0026rdquo; Select the core distribution and RAM allotment that makes sense for your purposes. For Machine, select \u0026ldquo;i440fx-6.1.\u0026rdquo; For Bios, select \u0026ldquo;SeaBIOS.\u0026rdquo; For USB Controller, select \u0026ldquo;2.0 (EHCI).\u0026rdquo; For Primary vDisk Location, select \u0026ldquo;Manual\u0026rdquo; and paste the location of your USB from earlier (e.g., \u0026lt;strong\u0026gt;/dev/disk/by-id/usb-_USB_DISK_3.0_xxxxxxxxxxxxxxxxxx-0:0\u0026lt;/strong\u0026gt;). For 2nd vDisk Location, select Auto or define your vDisk as needed (this will be the Persistence storage disk), and select the 2nd vDisk BUS to be \u0026ldquo;VirtIO\u0026rdquo;. See the image below.\nFinally, select a Network Bridge to use. Bonus points if you route an Unraid Network Bridge through a VLAN tagged subnet routed through a VPN (see an older post here for ideas, especially if you use Unifi).\nSave the VM, but do not start it upon saving!\nLaunch the VM Back on Unraid\u0026rsquo;s VM Manager page, click your VM, click Start, and very quickly select VNC Remote as soon as it is available. Once the VM loads, quickly press \u0026ldquo;Tab\u0026rdquo; on your keyboard to modify boot options. You will be presented with the following screen.\nModify the boot options by using your arrow keys, and use Backspace or Delete to remove the \u0026ldquo;live-media=removable\u0026rdquo; and \u0026ldquo;nopersistence\u0026rdquo; options. Note: You will need to do this every time you restart the VM. There is probably a way to make this permanent, but I have yet to explore that.\nOnce finished, click Enter twice, and Tails will begin to boot.\nSetup Persistence When presented with the \u0026ldquo;Welcome to Tails!\u0026rdquo; screen, select \u0026ldquo;Start Tails.\u0026rdquo; Then, choose \u0026ldquo;Applications\u0026rdquo; in the top left, select \u0026ldquo;Configure persistent volume\u0026rdquo; and follow the steps presented. The 2nd vDisk you created earlier will likely be pre-selected as mine was.\nFollow the steps to set up the volume, make a strong encryption key, and save. Restart Tails. Follow the same procedural steps to launch the VM as above while being sure to remove the boot options specified.\nOnce Tails has booted again, you will be presented with the same wlecome screen from earlier with the addition of an \u0026ldquo;Encrypted Persistent Storage\u0026rdquo; box as shown in the image below. Type in your strong encryption key, and click \u0026ldquo;Unlock.\u0026rdquo; From there, you have a fully functioning Tails VM with encrypted Persistent storage on Unraid!\nWrapping Up Congratulations if you made it this far and everything is working! I hope this tutorial aids you in your endeavors to use Tails as a VM. If you have any questions or need assistance, please let me know in the comments!\n","permalink":"https://whitematter.tech/posts/tails-os-with-encrypted-persistence-on-unraid-as-a-vm/","summary":"This post will show you how to run Tails OS as a VM with the Persistence feature enabled.\nRunning Tails as a VM is not recommended generally as it defeats many of the security features in Tails. For example, virtualization requires that you trust the hypervisor host, as the hypervisor has extra privileges over a VM that can reduce security and privacy of the VM.\nI recommend reading Tails\u0026rsquo;s official documentation about virtualization considerations before continuing: https://tails.","title":"Tails OS with Encrypted Persistence on Unraid as a VM"},{"content":"If you game on PC, you probably have Discord installed to chat with your friends as you game. Discord has relatively decent audio, and it far surpasses the in-game chat capabilities built into most games.\nWhen gaming cross-platform, however, you\u0026rsquo;re stuck either using in-game chat or the tedious Xbox Companion app with limited controls and annoying party configs.\nThis post will show you how to configure a setup to forward audio from an Xbox party directly to Discord, and visa versa. This way, you can game with your Xbox friends and still use the far superior Discord.\nI am running this setup on a Windows 10 VM on Unraid. I recommend using a VM on another PC or server that you are not gaming on for this setup.\nThe Virtual Audio Cable The Virtual Audio Cable (VAC) is the magic sauce for this setup. The biggest bummer with the VAC is that a home license is a $30 one time purchase. I went ahead and purchased the VAC because it is super useful in other applications too.\nThe VAC has a trial, but after 30 minutes, a voice talks in the background reminding you it is a trial.\nShould you try the trial or purchase the software, please continue reading!\nThe setup is straightforward. You need to create two cables and click \u0026ldquo;Set\u0026rdquo; in the top left corner of the software. See the image below for how it should look. Basically, this will create two audio interfaces that can be used to route audio to different places in your OS.\nOnce you have the two cables created, you\u0026rsquo;re done with this part!\nThe XBox Console Companion App Currently, I am using the Xbox Console Companion App. I believe at this time that Microsoft is attempting to push this application by the wayside for their newer Xbox PC app, but I still prefer the Xbox Console Companion at this time.\nBasically, you will want to create a new Xbox Live user that can be used to host Xbox parties with your Xbox friends. For example, you could create a user Xbox2Discord. Add all your friends who will want to cross-platform play with you.\nIn the app settings, select the Speaker source as \u0026ldquo;Line 1 (Virtual Audio Cable)\u0026rdquo;. For microphone, select \u0026ldquo;Line 2 (Virtual Audio Cable).\u0026rdquo; See the image below.\nThe Xbox setup is simple too. Once the above is completed, you are ready to move over to Discord.\nDiscord For the Discord setup, I likewise recommend creating a new user, Xbox2Discord or something of the like, for this setup. Add this user to whatever server you would like the functionality to exist.\nIn Voice \u0026amp; Video settings in the Discord app, change the Input device to Line 1 (Virtual Audio Cable) and select Line 2 (Virtual Audio Cable). See the image below for the example.\nOnce the above is completed, you can use your new user to join your chat in your Discord server. Once your friends join the Xbox Live party of your Xbox user, you should now be able to hear and talk to your Xbox friends via Discord! See the image below.\nConclusion Congratulations if you made it this far and everything is working! I hope this tutorial aids you in your endeavors to improve your social gaming experience. If you have any questions or need assistance, please let me know in the comments or email me at robert@whitematter.tech !\nSpecial thanks to Samuel Wallace for dreaming with me and making the ins-and-outs of this project work.\n","permalink":"https://whitematter.tech/posts/xbox2discord-how-to-forward-audio-from-xbox-live-to-discord/","summary":"If you game on PC, you probably have Discord installed to chat with your friends as you game. Discord has relatively decent audio, and it far surpasses the in-game chat capabilities built into most games.\nWhen gaming cross-platform, however, you\u0026rsquo;re stuck either using in-game chat or the tedious Xbox Companion app with limited controls and annoying party configs.\nThis post will show you how to configure a setup to forward audio from an Xbox party directly to Discord, and visa versa.","title":"Xbox2Discord: How to Forward Audio from Xbox Live to Discord"},{"content":"Introduction Recently, I had the opportunity to collaborate with a university research lab to build some vapor sensors to roughly measure ethanol (EtOH) vapor within an operant chamber. This project was a lot of fun.\nWith extremely limited circuit documentation on the web and never having personally used Arduino before, there were a lot of interesting hiccups I ran into. Overall, this project was a bit out of my wheelhouse, but with much determination, the finished product turned out rather nicely.\nThis project was made possible and open-source by the work of Dr. Brian McCool. The Arduino code hosted on GitHub, the general project design (along with most of the corresponding recommended parts list), and the conceptual use of the project for alcohol research are his work.\nThe Lab This project was completed for the Reward and Addictive Disorders (RAD) Lab at Miami University by Dr. Anna Radke.\nThe sensors will be used in a current experiment utilizing mouse operant chambers for a visual representation of the EtOH vapor pressure in the chambers. The support of this experiment utilizing these modules is under the direction of Elizabeth Sneddon (PhD candidate at the time of this writing; recipient of the prestigious DSPAN F99/K00 from NINDS \u0026ndash; read more from the Lab News here).\nI highly recommend checking out the RAD Lab\u0026rsquo;s website for more information on Dr. Radke and both her incredibly talented graduate students and undergraduate students. You can find information about many of their current projects.\nShameless plug: You can also check out a paper from the RAD Lab of which I am an author: https://pubmed.ncbi.nlm.nih.gov/30431655/\nFirst Successful Unit\nThe Parts Per Module Quick List:  Arduino Uno R3 Arduino Proto Shield for Arduino Kit (Stackable) Clear Enclosure for Arduino (adafruit.com) Standard LCD 16x2 screen (adafruit.com version) (amazon.com pre-soldered jumpers) i2c/SPI character LCD backpack MQ-3 alcohol gas sensor (sparkfun.com) Power supply (see comments below) Standard USB printer cable 500Ohm Trim Potentiometer (for rat operant boxes); 1k-2kOhm Trim Potentiometer (for mouse operant boxes) \u0026ndash; this kit contains 4 of each along with other (unnecessary for this project) resistance values Wiring: while there are many options for wires, this kit has an assortment of Male-Female, Male-Male, and Female-Female wires that will be helpful in learning/testing the circuit later Optional: Breadboard (this is super helpful to learn/test the circuit before you commit to soldering, but it is definitely not a requirement) Optional: This project REQUIRES soldering. I used this soldering iron kit here. In addition to this soldering iron, a solder sucker kit is highly recommended when you make a mistake.  Power supply notes: For the power supply, you can theoretically use a 5V 2A power supply, but I did not have luck with this option. For some reason, the voltage would drop to 3V on the Arduino unit when powered via this method. 3V is insufficient to run the readout on the screen, and I did not end up using this method for the finished product. If you want to test for yourself or know of some way to make this work, you could consider this 5V 2A switching power supply.\nOtherwise, I highly recommend powering your Arduino unit with a standard USB printer cable and a USB power block. Your mileage with that particular power block may vary, as it is specifically 5V and 2.1A. I did not use this particular power block but instead used an Apple power block for an iPhone. Any block that supplies at minimum 5V and 2A should be sufficient.\nNonetheless, you will need at least one standard USB printer cable in order to transfer the program to the Arduino device for the readout. This is covered in the Programming section.\nThis list is adapted from a list shared by Dr. Brian McCool.\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nThe Circuit This circuit diagram was drawn by my father, a radio and broadcast engineer, and I am very appreciative of his work and direction with this particular project. Hopefully this drawing can be useful to you or a technically-inclined student in your lab. Drawn by RW 09/04/2021\nArduino with MQ-3B Sensor Module Wiring Diagram\nSee or download the diagram here https://drive.google.com/file/d/1gVgyCg9lm8LaurHVQtRFqXGifgDlCiEb/view?usp=sharing OR here https://github.com/RobertDWhite/arduino_mq-3b\nAssembling the Arduino The order of completing these steps does not necessarily matter, but this is the order I completed them.\nI am certainly no Arduino expert. In fact, this project was my first experience with Arduino. I definitely had a fun (sometimes irritating) time building the modules. Unfortunately, I did not take as many pictures as I should have of the process or the built circuit (mostly because I did not intend to write this up). If I have an opportunity to build more in the future, I will update this post with more / better pictures.\nPreparing the Protoshield and Connecting to the Mainboard Assembling the Protoshield will require some soldering. You will see the many components included with the Protoshield nearly immediately. These components should be assembled to look like the image below. Specifically not the location of the capacitors, resistors, switches/buttons, and LEDs. The components can be soldered from the bottom of the board.\nThe black pinouts should be matched and aligned as shown in the picture above as well. Those should also be soldered from the bottom (see picture below for clarity), and they will be used to align the Protoshield to the Mainboard. The pins soldered to the Protoshield will insert into the pinouts on the Mainboard.\nOnce complete, your Protoshield and Mainboard should look like the first image above.\nConnect the LCD Backpack to the LCD Screen The 16-pin connector will be used to connect the backpack to the screen. The front of the screen should be soldered using the short ends of the connectors (see image below). Solder all 16-pins.\nProtip: Attach the 5-screw pins to the backpack before continuing. Once the backpack is attached to the screen, adding the screw pins is very very difficult.\nFrom the other side, attach the backpack and solder all 16-pins as shown in the image below.\nThe GND, 5V, CLK, and DAT ports are shown on the Diagram as the READ-OUT BOARD.\nWire the Protoshield Unfortunately, I do not have too many detailed images of the Protoshield with the wires attached. In the image below, you can see how I wired the 5V and the Ground from the Mainboard to the central 5V and GND. From there, all the required 5V and Ground wires can be soldered to the center for easy access.\nWhen wiring, I recommend using one of the breadboards listed as optional in the parts list. As you can see in the image below, you can test your wiring before making it more permanent with solder. Some parts can and should still be soldered (e.g., the screen and the backpack), but you will not regret testing the wiring before committing to solder.\nSuccessful Test Readout with Breadboard\nCleaned Up Wiring\nProgramming The code provided at GitHub was provided by Dr. McCool. Download the code to your computer. I highly recommend this post on how to upload the code to your Arduino. Once you have the connections as described above with your uploaded program to the Arduino, your screen should read-out the voltage.\nFine-Tuning the Read-Out The backpack has a small circular potentiometer built onto it near the top-right corner. The potentiometer is labeled \u0026ldquo;Contrast.\u0026rdquo; If your screen lights up but you do not see a read-out, you may need to turn the potentiometer to adjust the contrast on the screen. I had to adjust the contrast on each one of the modules I assembled.\nCaution: As shown in the image below, the potentiometer was easily displaced and broken. Be very careful when turning the potentiometer so you do not experience the same dilemma I did. This required a complete rewire for this particular module with a new external potentiometer to adjust contrast. I do not have a wiring schematic to share for this at this time, and if you happen to break the potentiometer, I recommend just getting a new backpack.\nBroken Backpack Potentiometer\nWrapping Up Congratulations if you made it this far and everything is working! I hope this tutorial aids you in your endeavors and can be a positive contribution to your research or personal projects. If you have any questions or need assistance, please let me know in the comments or email me at robert@whitematter.tech !\nFirst Successful Unit\nFirst Completed Unit\nUndershot of a Successful Build\n","permalink":"https://whitematter.tech/posts/arduino-mq-3b-ethanol-sensor-behavioral-neuroscience-research/","summary":"Introduction Recently, I had the opportunity to collaborate with a university research lab to build some vapor sensors to roughly measure ethanol (EtOH) vapor within an operant chamber. This project was a lot of fun.\nWith extremely limited circuit documentation on the web and never having personally used Arduino before, there were a lot of interesting hiccups I ran into. Overall, this project was a bit out of my wheelhouse, but with much determination, the finished product turned out rather nicely.","title":"Arduino MQ-3B Ethanol Sensor: Behavioral Neuroscience Research"},{"content":"Interestingly, after my previous post describing how to route Docker containers through VPN on Unraid, I received a substantial lot of questions via email about my hints at accessing Twitter anonymously. This post is my response to those questions, and I will describe my workflow to access Twitter feeds anonymously, without an account.\nThis post will assume you have read my post on how to route Docker containers through VPN on Unraid or that you already know how to accomplish this. If you do not, start here.\nI will be using Unraid in this tutorial, but you could accomplish the same functionality with Docker on another host OS.\nNitter The Docker container used to access Twitter is a wonderful third-party tool known as Nitter (zedeus/nitter). This container is relatively self-explanatory, but the purpose of the container is to be able to search for your favorite Twitter users and see their feeds.\nAll requests go through the backend, and your client never talks to Twitter, which is how Nitter prevents Twitter from tracking your IP or JavaScript fingerprint, thus enhancing privacy. Behind a VPN, you have yourself a self-hosted, anonymous Twitter reader that does not require an account or track you.\nThere are also public Nitter instances you could use if your prefer not to host your own instance: https://github.com/zedeus/nitter/wiki/Instances. But don\u0026rsquo;t you want to use your reverse proxy to route twitter.your-domain.com to your Nitter instance and have your own Twitter?\nExample Nitter Search for \u0026ldquo;Apple\u0026rdquo;\nSearch for your favorite user(s), select their profile, and click the RSS icon in the top right corner of the screen. Copy the URL of the RSS page for use in the next steps.\nClick the RSS Icon\nExample User Account on Nitter, of your favorite Elon Musk\nFreshRSS Now, in conjunction with your Nitter container behind a VPN (or simply a public Nitter instance like mentioned above), add FreshRSS (linuxserver/freshrss). This container will allow you to maintain the records of the feeds you want to follow in the form of RSS.\nEven if you choose to only use this container to access a locally hosted Nitter instance, I still recommend routing it through a VPN. There may come a day where you open your Nitter container publically or where you decide to grab RSS feeds from elsewhere. In either case, a VPN will help provide anonymity and security, making it even harder for trackers to store your data.\nOnce the container is launched, select \u0026ldquo;Add a feed or category\u0026rdquo; on the left navigation panel. Under \u0026ldquo;Add a category,\u0026rdquo; you can create your own categories to keep your feeds organized (e.g., Tech, Politics, etc.). Under \u0026ldquo;Add a feed,\u0026rdquo; you can begin adding the feeds from Nitter that you want to follow. In my screenshot below, you can see I am using a public Nitter instance and attempting to follow Elon Musk (https://nitter.fdn.fr/elonmusk/rss). Add the URL in the \u0026ldquo;Feed URL\u0026rdquo; section, select a category that you have made previously, and click \u0026ldquo;Add\u0026rdquo; to confirm. If you want to add even ANOTHER layer of privacy, you can also set up a SOCKS5 proxy connection as shown in the screenshot. If using Unraid to host your SOCKS5 proxy server, it would look something like the screenshot below.\nEven with FreshRSS, you can consider using your reverse proxy to route something like rss.your-domain.com to your container.\nI would highly recommend setting up HTTP auth via your reverse proxy if you plan to open either of these containers to the public. Strong authentication will be invaluable in such a case. As always, I recommend the use of a password manager in order to generate, store, and access strong credentials (I use 1Password currently).\nConclusion Congratulations if you made it this far and everything is working! I hope this tutorial aids you in your endeavors to anonymize your access to Twitter while substantially enhancing the privacy and preventing tracking of your personal data. If you have any questions or need assistance, please let me know in the comments or email me at robert@whitematter.tech !\n","permalink":"https://whitematter.tech/posts/how-to-access-twitter-without-an-account-anonymously/","summary":"Interestingly, after my previous post describing how to route Docker containers through VPN on Unraid, I received a substantial lot of questions via email about my hints at accessing Twitter anonymously. This post is my response to those questions, and I will describe my workflow to access Twitter feeds anonymously, without an account.\nThis post will assume you have read my post on how to route Docker containers through VPN on Unraid or that you already know how to accomplish this.","title":"How to Access Twitter Without an Account, Anonymously"},{"content":"I recently had to upgrade two domain controllers to Windows Server 2022. The main controller was still on Windows Server 2016 while the secondary was Windows Server 2019. Both in-place upgrades went without issue.\nBoth servers were running Hyper-V, Bitlocker, and AD Controller. The 2019 server had an SMTP server that it was running, but Microsoft has deprecated the SMTP stack and associated management tools, including the IIS tools. There very well may be other deprecated software or tools, and I highly recommend searching for specific functionality you utilize on your Win Servers before attempting the upgrade.\nReach out to my Microsoft Partner consulting company for the best prices on your Windows Server 2022 licenses and volume licenses. Otherwise, if you need a Microsoft Partner, hit us up. Visit wallaceandwhite.com or email us at info@wallaceandwhite.com\nInstructions  Backup your server. I used Veeam. Locate the Windows Server Setup media of the Win Server upgrade image, and then run setup.exe.  Select Yes to start the setup process. For internet-connected devices, select the Download updates, drivers and optional features [recommended] option, and then, select Next. Wait for the setup to check your device configuration, and then select Next. Depending on the distribution channel from where you received your Windows Server media (e.g., Retail, Volume License, OEM, ODM, etc.), you may be prompted to enter a licensing key to continue. Select the Windows Server edition you want to install and select Next. Select Accept to accept the terms of your licensing agreement, based on your distribution channel (e.g., Retail, Volume License, OEM, ODM, etc). Select Keep personal files and apps in order to do an in-place upgrade and then select Next.  After Setup analyzes your device, a prompt will appear to proceed with the upgrade by selecting Install.\nThe in-place upgrade starts and presents the Upgrading Windows screen showing its progress. After the upgrade finishes, the server will restart.\nConclusion It can certainly be intimidating to make these upgrades on production machines that your teams or business rely on daily, but Microsoft made this particular upgrade path pretty straightforward. As always, be sure you have a backup of your server before attempting such an upgrade. Nonetheless, you likely will not need it.\nCongratulations if you made it this far and everything is working! I hope this tutorial aids you in your endeavors to upgrade your servers to Windows Server 2022. If you have any questions or need assistance, please let me know in the comments or email me at robert@whitematter.tech !\n","permalink":"https://whitematter.tech/posts/how-to-do-an-in-place-upgrade-to-windows-server-2022/","summary":"I recently had to upgrade two domain controllers to Windows Server 2022. The main controller was still on Windows Server 2016 while the secondary was Windows Server 2019. Both in-place upgrades went without issue.\nBoth servers were running Hyper-V, Bitlocker, and AD Controller. The 2019 server had an SMTP server that it was running, but Microsoft has deprecated the SMTP stack and associated management tools, including the IIS tools. There very well may be other deprecated software or tools, and I highly recommend searching for specific functionality you utilize on your Win Servers before attempting the upgrade.","title":"How To Do An In-Place Upgrade To Windows Server 2022"},{"content":"Today\u0026rsquo;s post will cover how you can route any Docker container through a VPN.\nThere are many reasons you might want to route a Docker container through a VPN. Some common considerations are privacy, anonymity, and security.\nI always recommend a VPN provider that values privacy, and in your search, you should consider providers that do not keep access logs that can be tied back to you (I use Private Internet Access [PIA]).\nThere are many use-cases for this sort of functionality that do not entail breaking laws or piracy. I also DO NOT advocate for your use of these techniques for such purposes.\nFor example, one might be interested in anonymously accessing Twitter feeds in one container routed through a VPN, pulling those feeds into an RSS reader container, and anonymously hosting your own Twitter, anonymously.\nObtain VPN Container The first thing you will need to do is obtain a container on Unraid that can host your VPN client. For example, I use binhex/arch-privoxyvpn and highly recommend it. However, there are viable alternatives like NordVPN. You might want to do some research and see which best meets your needs. But, for PIA, binhex/arch-privoxyvpn is the best.\nCreate Docker Network for VPN Next, you will need to use the Terminal on Unraid to issue a command that specifically creates a Docker network that will use the VPN to route traffic through. Using Binhex\u0026rsquo;s PrivoxyVPN (where the container is named \u0026ldquo;privoxyvpn\u0026rdquo;), my command looks like this:\ndocker network create container:privoxyvpn Route Containers Through the New Network The next step is to route your containers through the new network you just created. To do this, go to the settings page of the container you would like to route and edit the \u0026ldquo;Network Type\u0026rdquo; on the container. Here, choose the new network you just created (e.g., privoxyvpn) (see image below).\nThen, you need to take note of and delete all port mappings on the container you just routed. Port mappings will either render the container useless or render the VPN useless. Delete the mappings and keep them for reference in the next step (for example, if you use the container \u0026ldquo;Nitter,\u0026rdquo; you would take note of and delete the port mapping to 8080).\nAfter the port mappings are deleted, all ports to your container will be routed through your VPN container.\nAdding GUI Ports to VPN Container Finally, in order to access your containers that are routed through the VPN, you must tell the VPN container what ports can be used for access.\nTo do this, go to your VPN container\u0026rsquo;s settings. In settings, add your ports to the VPN_OUTPUT_PORTS option (this may be different if you are not using Privoxyvpn). Then, map each port directly to the container by selecting \u0026ldquo;Add another Path, Port, Variable, Label or Device\u0026rdquo; at the bottom container\u0026rsquo;s settings page. Change \u0026ldquo;Config Type\u0026rdquo; to \u0026ldquo;Port\u0026rdquo;. Next, add the original port from your container under the \u0026ldquo;Container Port:\u0026rdquo; option. Then, add the port you would like mapped to the container to access that port under \u0026ldquo;Host Port:\u0026rdquo;. For example, if you want to access Nitter\u0026rsquo;s port 8080 on 8082, \u0026ldquo;Container Port:\u0026rdquo; will be 8080 and \u0026ldquo;Host Port:\u0026rdquo; will be 8082. Otherwise, you can make these two ports match for ease of setup. See the image below for an example.\nConclusion Once your container settings are saved, you will be able to access the GUI of your container, but the container traffic will be routed through your VPN! This will provide extra security, privacy, and anonymity for your data.\nCongratulations if you made it this far and everything is working! I hope this tutorial aids you in your endeavors to anonymize certain portions of your network. If you have any questions or need assistance, please let me know in the comments or email me at robert@whitematter.tech !\n","permalink":"https://whitematter.tech/posts/how-to-route-any-docker-container-through-vpn-in-unraid/","summary":"Today\u0026rsquo;s post will cover how you can route any Docker container through a VPN.\nThere are many reasons you might want to route a Docker container through a VPN. Some common considerations are privacy, anonymity, and security.\nI always recommend a VPN provider that values privacy, and in your search, you should consider providers that do not keep access logs that can be tied back to you (I use Private Internet Access [PIA]).","title":"How to Route Any Docker Container Through VPN in Unraid"},{"content":"Many of us have had the pleasure (or the curse) of working from home (better known as WFH on social media or your company Teams chat) over the COVID-19 pandemic. I, for one, welcomed the increased WFH time, but the lack of open and available coffee shops over the time was definitely a bummer (among other things, obviously).\nWhile designing cloud architecture or developing organizational NIST-compliant security policies, I require the aromatic scent of freshly ground espresso or single-origin coffee beans followed by an exquisitely crafted cortado or glass of nitro cold brew.\nIn order to keep my caffeine addiction live and well, I invested in some wonderful coffee gear that continue to enable my draw to our favorite competitive adenosine receptor antagonist. Not only can I try some really cool drinks, I have saved a ton of money by making my coffees and espresso-based drinks at home.\nIn this post, I will share my favorite gear in preparation for another post or two in the future about specific drinks I particularly enjoy.\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nEspresso First up is my espresso machine. I chose this specific machine because I wanted to retain some manual control of both pulling the shot and tamping, the machine has a decent grinder attached, and the machine is relatively cheap in its particular class.With this Breville espresso machine, I can quickly grind my favorite espresso bean, pull a relatively perfect shot, steam the right amount of milk, and craft the perfect drink.\nWhile making these perfect drinks, the process can get a little messy, and I prefer a rubber tamper mat to help keep the counter clean. The tamper mat is extremely easy to clean, and it saves quite the hassle of cleanup after a messy creation. Check that out below:\nA great use of your spent ground espresso (and coffee) is to fertilize plants (no, really!). I drink enough coffee and espresso to avoid purchasing fertilizer every year. Since using grounds as fertilizer, our plants are doing much better than before, as we have pretty bad soil overall. A great way to save the espresso puck specifically is a coffee knock box. I found a decent and cheap one here:\nCoffee Pour Over For pour over coffee, I use the Chemex and Chemex filters:\nThere are many different electric kettles you can choose from for pour over coffee. I have used many different electric kettles and prefer them to stove-top kettles because they are easier to control. My first choice is the kettle below because it allows for finite temperature control and it not bound to specific presets.\nThe electric kettle below is my second choice because I am also an avid tea drinker. The pre-set options to allow you to quickly change the temperature setting to a specific tea or coffee is particularly nice. I obviously like this for the exact opposite reason of my first kettle choice above. This particular selection is one I keep actually in my work office:\nFor pour over coffee, it is extremely important to get your measurements correct. A decent and affordable scale that was recommended to me by a barista and good friend from Three Ships Coffee in Virginia Beach is the model at the following link:\nVietnamese Coffee Vietnamese-style coffee with sweetened condensed milk is a pretty delicious drink. I particularly enjoy the iced coffee (cafe sua da) rendition. You can replicate the drip brew method with a cheap purchase of a Vietnamese coffee filter:\nFor Vietnamese coffee, I use the same kettles linked above.\nDrip Coffee For drip coffee, I had a pretty specific list of requirements that my machine needed to meet. Unfortunately, the search for one such device was more difficult than I envisioned.\nSpecifically, I wanted a machine with a decent built-in grinder. Having the ability to grind-on-demand is a pretty nice feature. Obviously I did not want a grinder that would skimp on quality and grind an inconsistent bean. My machine selection does a relatively decent job. The convenience it provides, to me, is absolutely worth the slight quality degradation that you may notice compared to a fancy standalone grinder.\nAdditionally, I wanted to be able to easily brew a small mug or a large carafe with ease. My machine selection actually allows one to brew a variety of mug sizes (single serve) as well as 1-12 cups in a carafe. Having flexibility with brew amount is very beneficial if you\u0026rsquo;re making coffee for one or making coffee for a group of friends.\nFinally, I wanted a machine that would allow me to program and schedule the machine the night before to start at a specific time. I definitely wanted to have freshly ground coffee in the morning, but time was extra tight pre-WFH with school and full-time work. My wife also loved to be able to have her coffee set at a specific time and appreciated fresh grounds.\nBreville saved the day again by meeting all of the above requirements. At the time of purchase, they were the only company to my knowledge with a machine that met these requirements. I am still (as of this post) unaware of any competition. Here is the wonderful machine that I highly recommend:\nCold Brew For cold brew, I have been consuming more than I can produce due to the 12-24 hour brew period. Because of this, I recently upgraded my 2-quart cold brew maker to a gallon mason jar with dispenser:\nNitro Cold Brew From the cold brew make above, I oftentimes transfer the cold brew coffee to a kegerator in order to infuse with nitrogen. The specific nitro cold brew maker I use is the following:\nIn order to infuse the coffee, you need nitrous oxide canisters. The canisters that I have found successful thus far are N2O canisters for whipped cream makers:\nSiphon Coffee Siphon coffee is probably a little more work than you would want for your daily cup, but it is definitely a crowd pleaser. It also provides one of the best full-body brew methods I have tried! My current siphon coffee maker is the following:\nThat siphon does a great job, but there are siphons that look a lot cooler. For example, the Belgian Family Balance Siphon looks legit. I avoided it because of the relative cost, though I would love to try it in the future. Comment below or shoot me a message if you have tried it!\nWrapping Up Clearly, there are a lot of different ways to brew coffee at home. I hope some of this list inspires you to try some new methods in your own home for yourself or your friends/family.\nPlease share your experiences with any of the products I have listed in this post or mention your favorite alternatives. I am always looking for better alternatives! Share your favorite method below, or shoot me an email at robert@whitematter.tech. Thanks for reading!\n","permalink":"https://whitematter.tech/posts/coffee-gear/","summary":"Many of us have had the pleasure (or the curse) of working from home (better known as WFH on social media or your company Teams chat) over the COVID-19 pandemic. I, for one, welcomed the increased WFH time, but the lack of open and available coffee shops over the time was definitely a bummer (among other things, obviously).\nWhile designing cloud architecture or developing organizational NIST-compliant security policies, I require the aromatic scent of freshly ground espresso or single-origin coffee beans followed by an exquisitely crafted cortado or glass of nitro cold brew.","title":"The Ultimate Coffee Gear List for WFH"},{"content":"For this post, I will show you how to easily run a Docker Registry GUI with Harbor. I am running Docker on a Ubuntu VM. Therefore, my registry will be run through Docker, and the container will reside on a Ubuntu VM. This tutorial will use docker-compose to build the required containers.\nDownload and Expand the Harbor Installer wget https://github.com/goharbor/harbor/releases/download/v2.3.1/harbor-offline-installer-v2.3.1.tgz tar -xzf harbor-offline-installer-v2.3.1.tgz cd harbor/ Generate SSL Certs - INTERNAL ONLY These steps should only be taken if you plan to use your registry internally. If you plan to host your registry for external access, you should obtain certs from a trusted CA to use. With that said, these steps will work on a Linux host only (if you are using Windows, you could use WSL to follow these steps).\nOpen up a Terminal and copy+paste the following commands. Be sure to replace all the red text with your own organization\u0026rsquo;s name and information.\nAdditionally, pay specific attention to the cat command where [alt_names] occurs. You will need to replace DNS.1, DNS.2, and DNS.3 to match the local IP or FQDN of the host machine. For example, if your machine\u0026rsquo;s name is \u0026ldquo;hostname\u0026rdquo; and your network domain is \u0026ldquo;.local,\u0026rdquo; you would want to use hostname.local and hostname as your DNS entries. If the machine\u0026rsquo;s IP is static, you could also include the IP. If you do not need all three DNS entries, you can delete whichever are not needed. Note: If you have trouble successfully generating the harbor/v3.txt file with the cat command for some reason, you can download a pre-created file at GitHub.\nOnce the certs are created, save them so that you can add the certs as \u0026ldquo;trusted\u0026rdquo; on whatever machines you plan to use to access the registry.\n Preparing for Install To simplify your install, I have hosted the docker-compose.yml I use to deploy my registry and harbor: GitHub. The docker-compose.yml file should go in the harbor directory created above. If you followed my commands above, your Terminal should already be in the correct directory. You can easily pull this docker-compose.yml with cURL or wget by running the commands below:\ncurl -LJO [https://raw.githubusercontent.com/RobertDWhite/harbor-registry/main/docker-compose.yml](http://curl -LJO https://raw.githubusercontent.com/RobertDWhite/harbor-registry/main/docker-compose.yml \u0026#34;https://raw.githubusercontent.com/robertomano24/nginx-proxy-manager/main/docker-compose.yml\u0026#34;) OR\nwget --no-check-certificate --content-disposition [https://raw.githubusercontent.com/RobertDWhite/harbor-registry/main/docker-compose.yml](https://raw.githubusercontent.com/RobertDWhite/harbor-registry/main/docker-compose.yml) If you prefer to create your docker-compose.yml file yourself, make a file named “docker-compose.yml” and add the contents of the GitHub link to the file.\nInstalling the Registry and Harbor Now, everything should be in place to run docker-compose and build-out successfully. To do this, run the following:\ndocker-compose up -d This may take a few minutes, but when you are finished, you should have successfully running containers for your Docker Registry and Harbor (the registry GUI). You should now be able to access the Harbor GUI from a web browser at https://hostname, https://hostname.local, or https://IP_OF_HOST if you have the SSL certs \u0026ldquo;trusted.\u0026rdquo;\nAdministrating Harbor I will not go into detail about the administration of Harbor, but I will point you to Harbor\u0026rsquo;s official documentation which contains a wealth of knowledge: https://goharbor.io/docs/2.3.0/administration/\nWrapping-Up If you were able to make it through all the steps above, you should be able to push to and pull from your registry. You will definitely want to look up information about how to tag images and containers in Docker.\n As always, if you have any questions, feel free to post below and share your successes or frustrations! You can also reach out to me at robert@whitematter.tech.\nThanks for reading!\n ","permalink":"https://whitematter.tech/posts/how-to-run-a-locally-hosted-docker-registry-gui-with-harbor/","summary":"For this post, I will show you how to easily run a Docker Registry GUI with Harbor. I am running Docker on a Ubuntu VM. Therefore, my registry will be run through Docker, and the container will reside on a Ubuntu VM. This tutorial will use docker-compose to build the required containers.\nDownload and Expand the Harbor Installer wget https://github.com/goharbor/harbor/releases/download/v2.3.1/harbor-offline-installer-v2.3.1.tgz tar -xzf harbor-offline-installer-v2.3.1.tgz cd harbor/ Generate SSL Certs - INTERNAL ONLY These steps should only be taken if you plan to use your registry internally.","title":"How to Run a Locally Hosted Docker Registry GUI with Harbor"},{"content":"Introduction For this post, I will show you how to setup Unraid to run AlienVault OSSIM as a VM. OSSIM is a powerful open-source SIEM that you can leverage on your network for free. I use OSSIM for network-wide vulnerability scanning and endpoint host intrusion detection.\nOSSIM\u0026rsquo;s integrated HIDS is a fork from OSSEC. Additionally, OSSIM integrates with Open Threat Exchange (OTX), which can be installed on Windows, Mac, and Linux endpoints and servers for an up-to-date, open-source vulnerability scanning tool. I deploy the OTX installer via my free Mosyle account (MDM for MacOS) and Intune (MDM for Windows).\nMy main server, I suppose, is a custom-built Unraid server. For a long time, I have hoped to run OSSIM on Unraid, but I was never quite able to figure out how to get the OSSIM VM to boot. I usually could get through the GUI installer, but once the post-install reboot occurred, I would always be met with a screen that would freeze and say \u0026ldquo;Booting from device\u0026hellip;\u0026rdquo; for the rest of eternity.\nI accidentally recently botched my OSSIM bare-metal install that was running on an old Dell workstation. It did a great job since resource requirements are relatively minimal for OSSIM. When I attempted to modify some install files on OSSIM via APT, however, I made a crucial mistake and decided to rebuild OSSIM from scratch. Revisiting the VM on Unraid configuration made sense again.\nThis time, I was able to find the correct combination of settings in the Unraid VM options to successfully load, install, and post-install boot the OSSIM OS. Maybe, overall, it was a lot easier than I made it originally, but I thought that documenting it would be beneficial, as I was unable to find any walkthroughs or tutorials for this specific setup.\nDownload OSSIM OSSIM is available for download for free at the following link: https://cybersecurity.att.com/products/ossim/download. OSSIM is free and open-source. If you download from the link, you will get an ISO that you can use to install your VM.\nAlternatively, you can SSH into your Unraid or you can use Unraid\u0026rsquo;s built-in Terminal to run the following command to download the ISO (be sure to cd into a share directory prior to downloading, for example, /mnt/user/iso):\nwget https://dlcdn.alienvault.com/AlienVault_OSSIM_64bits.iso Prepare Unraid If you downloaded the ISO to your computer, connect to a file share on your Unraid and transfer the ISO to Unraid. If you used wget, the ISO is already on Unraid ready to go.\nCreate a New VM The biggest trick to get OSSIM to work on Unraid is specifically choosing Debian as the VM host. Under the VMs tab on Unraid, select ADD VM. On this screen under Linux, select Debian.\nConfigure the VM To configure the VM, be sure to leave CPU mode as the default \u0026ldquo;Host Passthrough.\u0026rdquo; You can select the number of cores/threads you would like for your OSSIM VM. My CPU is a Ryzen 9 3950x with 16 cores / 32 threads. I currently have 8 cores assigned to my VM. I run 8192MB for both Initial and Max Memory. For the Machine, select Q35-6.0. For BIOS, select SeaBIOS. For USB Controller, select 3.0 (nec XHCI). For OS Install ISO, select the ISO you uploaded / downloaded earlier (e.g., /mnt/user/iso/AlienVault_OSSIM_64bits.iso).\nNext, select SATA as the OS Install CDRom Bus. Select where you would like the OSSIM VM virtual disk to be located and how large it should be under Primary vDisk Location (e.g., mine is located on a specific VM pool I made on an SSD). Select SATA again for Primary vDisk Bus. Leave everything else down to Network MAC as is. In the bottom left of the network pane, click the \u0026ldquo;+\u0026rdquo; to add another virtual network interface. For the new 2nd Network MAC, I recommend copy+pasting the original MAC and changing the last number/letter to be different. Leave the rest of the settings as is.\nStart the VM Save the configuration page and load the VM. From here, you should be able to follow the OSSIM install as you would in any other environment. The vDisk will be selected for install automatically unless you happen to add another disk above for your own configuration needs. When it comes time for the install to install the GRUB bootloader, it will fail. Continue with the install WITHOUT a bootloader when the install asks. Once the install completes and the VM restarts, you should be able to configure your OSSIM admin credentials!\nWrapping Up OSSIM is extremely powerful and can be complicated to use. If you are not already familiar with OSSIM, I recommend doing some intense Google searching about SIEM tools in general and specifically about configuration recommendations for OSSIM. It may be a little overkill for your Home Lab, but it is a valuable tool to be comfortable with for any security practitioner.\n As always, if you have any questions, feel free to post below or reach out to me at robert@whitematter.tech.\nThanks for reading!\nRobert\n ","permalink":"https://whitematter.tech/posts/how-to-run-alienvault-ossim-as-a-vm-on-unraid/","summary":"Introduction For this post, I will show you how to setup Unraid to run AlienVault OSSIM as a VM. OSSIM is a powerful open-source SIEM that you can leverage on your network for free. I use OSSIM for network-wide vulnerability scanning and endpoint host intrusion detection.\nOSSIM\u0026rsquo;s integrated HIDS is a fork from OSSEC. Additionally, OSSIM integrates with Open Threat Exchange (OTX), which can be installed on Windows, Mac, and Linux endpoints and servers for an up-to-date, open-source vulnerability scanning tool.","title":"How to Run AlienVault OSSIM as a VM on Unraid"},{"content":"Abstract This review investigated findability and its relationship to usability and usability evaluations. Definitions of usability, findability, usability evaluations, and other sub-components of these terms including navigability are discussed, and the importance of research studies using similar terminologies is considered. A series of six usability evaluation studies is presented, and the terminologies presented in each are described. Goals, findings, and conclusions of each study are considered, and applications from the conclusions are drawn. This review concludes that clear definitions of these topics are crucial to successful implementation of various study methodologies and presentation of results. The review further concluded that, based on the available research, findability is a critical factor of usability that, when effectively present, leads to greater usability and overall user satisfaction of a product or website.\nKeywords: Usability evaluations, findability, accessibility, usability\nIntroduction Usability Evaluations\nUsability evaluations are systematic reviews to evaluate usability. Trivedi \u0026amp; Khanum (2012) define usability as “the ability of a system to carry out specific tasks by specific users in a specific context” (p. 1). In this way, usability is concerned with how users interact and utilize a system to meet the objective of the system, given a specific circumstance or time. Usability evaluations, then, are concerned with evaluating how well usability is achieved given a particular system. Research studies and industry studies often utilize usability evaluations in order to explore usability of a system or product. Usability evaluations are often considered quick and relatively cheap means of evaluating usability (Trivedi \u0026amp; Khanum, 2012). Nonetheless, usability evaluations can quickly become complicated considering the multiplicity of available evaluation techniques, models, and usability factors that are involved. Additionally, because of this highly complex nature, it is difficult for inter-study comparisons to be completed, leading to confusion in the literature when comparisons are attempted (Hartson et al., 2001). Researchers have argued that a standardized method or model should be developed for use in usability evaluations, but studies remain diverse in their selection of methodology, models, techniques, and even factor definitions.\nThere are many models and techniques that can be used to frame usability evaluations. However, some models are more common or usable than others. A common model used in modern research is the Nielson Model, named after a key figure in the development of usability studies (Muqtadiroh et al., 2017). The Nielson Model evaluates a specific set of usability factors, conceptualized by results of rigorous research. These conceptualized usability factors in the Nielson Model include findability, learnability, efficiency, memorability, few errors, and user satisfaction (Muqtadiroh et al., 2017). Certain usability evaluation techniques include heuristic evaluation, cognitive walkthroughs, and action analyses (Trivedi \u0026amp; Khanum, 2012). Other models and techniques include other usability factors. Importantly, there are many factors that can be evaluated, and each of these factors often have multiple measures (Banker, 2020). Usability studies thrive on the interplay among these factors and the techniques which are used to obtain the data about the factors. Conceptualizing and defining the terms, models, and used techniques are important for clear communication, understanding, and successful usability evaluations.\nFindability\nFindability, as mentioned above, is a usability factor that is specifically examined in usability evaluations. Findability represents how findable something is (Banker, 2020). In the context of usability evaluations, findability is often concerned with how findable particular data are, but findability could be concerned with finding any aspect of a system or product. Findability has been measured in a variety of ways overall, but a prominent set of measures of findability are navigability and retrievability (Wilkie \u0026amp; Azzopardi, 2013; Banker, 2020). Wilkie \u0026amp; Azzopardi (2013) describe navigability as how easily it is to navigate a site’s link structure while looking for a specific page, and they describe retrievability as how easily a user can retrieve a page using a search engine. Further definitions of retrievability emphasize that retrievability is focused on what, how, and whether a user is likely to retrieve something or not instead of simply relevance (Azzopardi, 2015). While these particular definitions tend to lean in the direction of website or webpage findability, the concepts could be used in other domains as well.\nPurpose\nThe purpose of this review is to analyze usability evaluation studies and explore specifically the findability usability factor in the literature. This review will address usability evaluations and their effectiveness and the role of findability in these studies. For the purpose of this review, usability evaluations seek not only to understand the current state of usability of a particular system or product but also to improve the usability of that system of product for a better overall user experience (Trivedi \u0026amp; Khanum, 2012). Review Navarrete \u0026amp; Lujan-Mora (2015)\nA usability evaluation was conducted to examine the usability of e-Education collaborative environments which utilize Open Education Resources (OER) (Navarrete \u0026amp; Lujan-Mora, 2015). Researchers describe the importance of OER in the changing educational landscape of the past decade but emphasize the often-lacking accessibility due to issues of findability. Because of these findability issues, OER can actually become barriers, specifically for users with disabilities, for the accessibility of e-Educational resources which aim to increase the accessibility of these resources to greater numbers of users. This team defines findability as how easy it is to find content or functionality that the user expects on a particular site, which would be specifically OER sites in this case. In order to examine findability, Navarrete \u0026amp; Lujan-Mora (2015) explored three measures of findability, namely web accessibility, web usability, and information architecture. Evaluation of each measure differed, but researchers defined a low score in any of the measures as detrimental to the findability of the examined OER. Researchers conducted the usability evaluation focusing on findability of educational resources from the perspective of users with disabilities utilizing a set of 247 guidelines extracted from international standards (i.e., ISO). Navarrete \u0026amp; Lujan-Mora (2015) found that each site they examined, as well as many of the educational resources available on these sites, presented findability concerns for users with disabilities, suggesting that each site should be re-evaluated in order to increase the intended accessibility. A proposed takeaway from this research is to increase awareness of usability factors, specifically findability, in these OER sites during development, evaluation, and modification (Navarrete \u0026amp; Lujan-Mora, 2015). This study highlights how findability can directly impact accessibility of resources for different groups.\nShieh (2012)\nA usability evaluation study to evaluate the usability and findability of electronic library resources was conducted (Shieh, 2012). In a similar goal to the aforementioned study, this study aimed to evaluate the usability of these electronic library resources to improve accessibility through website usability and specifically to internal website resource findability. The study author defines effective findability as the ability to easily find a user’s required information both quickly and intuitively (Shieh, 2012). The study methodology specifically entailed an interesting approach to reconstruct the accessed library websites and resources using user logs and sub-sessions. In this way, the log information of the actual sessions indicating how a user was using a site at the time were used to reconstruct the sites, which emphasizes the actual use-case of the site overall. The reconstructed library sites were used to evaluate overall findability using a heuristic method. Using this log recreation model followed by heuristic evaluation, Shieh (2012) found increased findability on the reconstructed web pages. This novel model could be used in future research for research or business teams hoping to increase findability on a particular web page.\nAuinger et al. (2012)\nAuinger et al. (2012) conducted a theory-based usability evaluation to examine how findability and usability interact with search engine optimization (SEO) for business success. Researchers in this particular study fail to adequately define findability, which was noted earlier in this review to be a concern with these types of studies. However, the researchers emphasize the functional importance of findability as the most critical factor for effective usability and accessibility. Auinger et al. (2012) define usability per the ISO Standard 9241. Researchers utilized a scenario-based usability evaluation consisting of four different scenarios to examine the effects of the different kinds of SEO-optimization techniques defined in the study. Each scenario included an SEO-optimized version of the site to be studied as well as the current, live, publicly accessible version of the site. Users participated in the scenarios and answered questions concerning the use of the sites in their respective trials. Researchers found a positive effect of SEO-methods on site usability while finding negative effects on usability when SEO-problems were present (Auinger et al., 2012). Researchers conclude that websites with SEO-methods applied are more usable and lead to higher satisfaction due to greater overall content and information findability. Overall, this study should inform other investigators and business teams to consider SEO-methods to increase website findability and overall usability.\nCho et al. (2019)\nA study designed to examine usability and accuracy of a newly developed pill identifier application that can be used on smartphones was conducted (Cho et al., 2019). This study differs from the previous studies evaluated because it does not directly consider findability as a crucial component of the product. Rather, findability is simply one of six measured facets of usability from the honeycomb model. An experimental design to compare the new image-processing based pill identifier application to the existing conventional pill identifier application and usability evaluation yielded data suggesting increased findability of the image-processing based pill identifier application (Cho et al., 2019). Researchers conclude that, overall, accuracy was not statistically different between the new application and the existing application, but usability of the new application was nonetheless good when used by medical personnel. The researchers suggest that usability for this application needs to be extended to other groups in the future for greater usability between groups. Nonetheless, findability was greater in the new application, and these results may impact the suggested choice of application for medical professionals in the future (Cho et al., 2019). The study results differ from the previous studies as well in that it is the only study evaluated thus far where increased findability did not correlate with overall increased usability. This particular study can be used as support for selecting the new application for medical professionals in settings requiring use of a pill identifier application.\nSimunich et al. (2015)\nAnother study was conducted to examine findability and its importance in online student perceptions of satisfaction in online courses (Simunich et al., 2015). The purpose of this exploration was to determine if findability should be more greatly emphasized in online course design. Simunich et al. (2015) quote Morville (2005) for their definition of findability, which is the following: “the degree to which a particular object is easy to discover or locate, [as well as] the degree to which a system or environment supports navigation and retrieval” (p. 174). Researchers emphasize the foundational importance of findability in usability and usability evaluations, and they consider findability as crucial to online educational environments. Simunich et al. (2015) chose two online courses for control groups and compared them to modified versions of both courses. The modified courses consisted of the same content with altered design specifically tailored to go against usability standards. The researchers intentionally violated the usability standards for the experimental groups. Participants were randomly assigned to either one of the control groups or one of the experimental groups. In addition to one of these groups, users were assigned to either an eye-tracking group or a focus group for data gathering. Researchers found that findability predicted self-efficacy and motivation in online courses, where greater findability led to overall greater satisfaction in the online courses (Simunich et al., 2015). Researchers conclude that this research may be useful in defining minimum findability requirements for online courses in the future in order to lead to greater usability and overall student satisfaction, self-efficacy, and motivation in online courses.\nSamuel et al. (2012)\nA final study examined the usability of health information websites and findability of information on those sites (Samuel et al., 2012). Researchers in this study define findability as “the ease of locating information on a website” (p. 709). Researchers were specifically interested in the aspect of findability concerning content on the health information sites. Researchers identified a few health information sites to evaluate user interaction. Methodology included usability evaluation and online surveys with tree testing. Assets on the health information sites, specifically the search box, navigation menu, and home page were evaluated for usability and information findability using those assets. Samuel et al. (2012) found that the search box is the primary way that users find information on these health information sites. The navigation menus and links are seldom used. Because of this, Samuel et al. (2012) conclude that the search box should be the most prominent asset visible on the health information sites, especially on the home page which represents the first interaction a user will likely have with a particular site. Researchers suggest a variety of ways to improve navigability (discussed briefly in this review’s introduction) including adding faceted search to the search box. These recommendations are thought to improve findability, but more research will be needed in order to support these assertions. This study could be used to both support researchers and business professionals in their development of usable sites involved with information dissemination. Findability and emphasis on usable search tools should be highlighted in these cases.\nConclusion This review evaluated the concept of usability evaluations and the foundational component of findability. Findability is a crucial usability factor that, when present and emphasized in effective ways, leads to more usable products and websites. The studies examined provide insight into a high-level understanding of how these usability evaluations are considered and approached. They also provide insight into the relationship of findability and usability, where greater findability tends to lead to greater usability and overall user satisfaction. The importance of considering definitions and conceptualizations of the various terms involved is evident when reviewing the literature. While many studies use similar definitions of findability, different definitions can lead to slightly different understandings of the importance of findability in these usability evaluations, which may lead to completely different approaches, biases, and considerations when designing studies. References Auinger, A., Brandtner, P., Großdehner, P., \u0026amp; Holzinger, A. (2012). Search engine optimization meets e-business-A theory-based evaluation: Findability and usability as key success factors. In ICE-B 2012: International Conference on e-Business and Telecommunications (pp. 237-250). Azzopardi, L. (2015, March). A Tutorial on Measuring Document Retrievability. In European Conference on Information Retrieval (pp. 813-816). Springer, Cham.\nBanker, D. A. (2020). Findability, Accessibility, and Usability of Data Portals in Education. Journal of Education \u0026amp; Social Policy, 7(1), 8-15.\nCho, S. K., Kim, B., Park, E., Kim, J., Ryu, H., \u0026amp; Sung, Y. K. (2019). Usability Evaluation of an Image-based Pill Identification Application. Journal of Rheumatic Diseases, 26(2), 111-117.\nFindlater, L., \u0026amp; McGrenere, J. (2007, September). Evaluating reduced-functionality interfaces according to feature findability and awareness. In IFIP Conference on Human-Computer Interaction (pp. 592-605). Springer, Berlin, Heidelberg.\nHartson, H. R., Andre, T. S., \u0026amp; Williges, R. C. (2001). Criteria for evaluating usability evaluation methods. International journal of human-computer interaction, 13(4), 373-410.\nMatera, M., Rizzo, F., \u0026amp; Carughi, G. T. (2006). Web usability: Principles and evaluation methods. In Web engineering (pp. 143-180). Springer, Berlin, Heidelberg.\nMorville, P. (2005). Ambient findability. Sebastopol, CA: OReilly Media.\nMuqtadiroh, F. A., Astuti, H. M., Darmaningrat, E. W. T., \u0026amp; Aprilian, F. R. (2017). Usability evaluation to enhance software quality of cultural conservation system based on nielsen model (wikibudaya). Procedia Computer Science, 124, 513-521.\nNavarrete, R., \u0026amp; Lujan-Mora, S. (2015, April). Evaluating findability of open educational resources from the perspective of users with disabilities: a preliminary approach. In 2015 Second International Conference on eDemocracy \u0026amp; eGovernment (ICEDEG) (pp. 112-119). IEEE.\nRedish, J., Bias, R. G., Bailey, R., Molich, R., Dumas, J., \u0026amp; Spool, J. M. (2002, April). Usability in practice: formative usability evaluations-evolution and revolution. In CHI'02 extended abstracts on Human factors in computing systems (pp. 885-890).\nSamuel, H. W., Zaïane, O. R., \u0026amp; Zaïane, J. R. (2012, January). Findability in health information websites. In Proceedings of 2012 IEEE-EMBS International Conference on Biomedical and Health Informatics (pp. 709-712). IEEE.\nShieh, J. C. (2012). From website log to findability. The Electronic Library.\nSimunich, B., Robins, D. B., \u0026amp; Kelly, V. (2015). The impact of findability on student motivation, self-efficacy, and perceptions of online course quality. American Journal of Distance Education, 29(3), 174-185.\nTrivedi, M. C., \u0026amp; Khanum, M. A. (2012). Role of context in usability evaluations: A review. arXiv preprint arXiv:1204.2138.Wilkie, C., \u0026amp; Azzopardi, L. (2013, March). An initial investigation on the relationship between usage and findability. In European Conference on Information Retrieval (pp. 808-811). Springer, Berlin, Heidelberg.\n","permalink":"https://whitematter.tech/posts/usability-evaluations-findability/","summary":"Abstract This review investigated findability and its relationship to usability and usability evaluations. Definitions of usability, findability, usability evaluations, and other sub-components of these terms including navigability are discussed, and the importance of research studies using similar terminologies is considered. A series of six usability evaluation studies is presented, and the terminologies presented in each are described. Goals, findings, and conclusions of each study are considered, and applications from the conclusions are drawn.","title":"Usability Evaluations: Findability"},{"content":"The ability for geographically dispersed teams to interact, engage, and collaborate effectively is a concern for increasing numbers of corporations, teams, and leadership, especially in the post-COVID-19 era workplace. Adoption of technological solutions for computer-mediated communication can mitigate some of the inherent complexities and difficulties of the geographical dispersion problem, and research teams have increased efforts to both understand the problem and design groupware solutions to support virtual team collaboration (Morrison-Smith \u0026amp; Ruiz, 2020). Researchers suggest that, along with the physical demands of distance for virtual teams, leaders should consider cognitive, social and emotional concerns at the forefront of the discussion in order to best address the problems and create a positive collaborative environment (Morrison-Smith \u0026amp; Ruiz, 2020).\nA key component to success for organizations facing a growing virtual team environment is to train managers and leaders with the skills necessary to lead employees in online, virtual environments (Ford et al., 2017). The key to success in these virtual team environments is trust (Ford et al., 2017; Newman et al., 2020; Zakaria \u0026amp; Yusof, 2020), and the development of theoretical swift trust formation processes have been recent topics of research development (Zakaria \u0026amp; Yusof, 2020). In virtual teams, leaders should emphasize the development of trust with their employees, which helps mitigate psychological and social feelings of isolation that are often present in virtual environments. Researchers recommend that intentional actions be taken to promote trust and relationship building to mitigate the task-oriented behaviors that often are present leading to a different form of trust, namely cognitive-based trust (Zakaria \u0026amp; Yusof, 2020). While this form of trust can help build long-term trust within a team, interpersonal trust is necessary for swift trust formation and the long-term trust that can be fostered (Zakaria \u0026amp; Yusof, 2020).\nFinally, researchers emphasize the importance of empowering leadership for virtual teams, defined by the empowering leadership theory (Bell \u0026amp; Kozlowski, 2002; Hill \u0026amp; Bartol, 2016). Empowering leadership is characterized by leadership behaviors emphasizing shared power between leaders and subordinates. These types of behaviors lead to increased intrinsic motivation in subordinates. Empowering leadership fosters supportive environments for subordinates to have opportunities to leverage the power entrusted to them (Hill \u0026amp; Bartol, 2016). Leadership training in empowering leadership is a viable option to encourage the development of empowering leadership practices in leaders, especially if an organization tends toward virtual or dispersed team environments. Hill \u0026amp; Bartol (2016) suggest that, the more dispersed a team is, the greater the positive impact of successful empowering leadership is.\nReferences\nBell, B. S., \u0026amp; Kozlowski, S. W. (2002). A typology of virtual teams: Implications for effective leadership. Group \u0026amp; organization management, 27(1), 14-49.\nFord, R. C., Piccolo, R. F., \u0026amp; Ford, L. R. (2017). Strategies for building effective virtual teams: Trust is key. Business Horizons, 60(1), 25-34.\nHill, N. S., \u0026amp; Bartol, K. M. (2016). Empowering leadership and effective collaboration in geographically dispersed teams. Personnel psychology, 69(1), 159-198.\nMorrison-Smith, S., \u0026amp; Ruiz, J. (2020). Challenges and barriers in virtual teams: a literature review. SN Applied Sciences, 2, 1-33.\nNewman, S. A., Ford, R. C., \u0026amp; Marshall, G. W. (2020). Virtual team leader communication: Employee perception and organizational reality. International Journal of Business Communication, 57(4), 452-473.\nZakaria, N., \u0026amp; Yusof, S. A. M. (2020). Crossing cultural boundaries using the internet: Toward building a model of swift trust formation in global virtual teams. Journal of International Management, 26(1), 100654.\nEdit this page on GitHub","permalink":"https://whitematter.tech/posts/a-quick-note-on-leading-geographically-dispersed-teams/","summary":"The ability for geographically dispersed teams to interact, engage, and collaborate effectively is a concern for increasing numbers of corporations, teams, and leadership, especially in the post-COVID-19 era workplace. Adoption of technological solutions for computer-mediated communication can mitigate some of the inherent complexities and difficulties of the geographical dispersion problem, and research teams have increased efforts to both understand the problem and design groupware solutions to support virtual team collaboration (Morrison-Smith \u0026amp; Ruiz, 2020).","title":"A Quick Note on Leading Geographically Dispersed Teams"},{"content":"Alright, if you have a Unifi device like a Dream Machine, Dream Machine Pro, UNVR, CloudKey, or other device, you likely have been met with the dreaded red triangle followed by the tedious words, \u0026ldquo;Your connection is not private.\u0026rdquo;\nFor some reason, at the time of this writing, there is still no official/built-in way to generate a cert for your device, though some interesting GitHub projects exist that claim to allow this functionality if you\u0026rsquo;re feeling lucky. My solution is not quite as glamorous as directly generating certs with LetsEncrypt server-side like the GitHub project linked above, but it will definitely get the job done!\nFor my solution, I have a NGINX reverse proxy setup to forward traffic from a subdomain associated with my domain (e.g., unifi.whitematter.tech, protect.whitematter.tech) to either my Dream Machine Pro or my UNVR as necessary. For steps on setting up a reverse proxy using Docker, check out my previous post here.\nOnce your reverse proxy is setup and you have added a subdomain to your DNS records (e.g., unifi.whitematter.tech) to point to your public IP address, you are ready to configure your reverse proxy.\nThis particular configuration assumes that you have forwarded your subdomain (e.g., unifi.whitematter.tech) to your public IP. If you do not have a static public IP address, you might consider a free dynamic DNS service like DuckDNS, which will automatically update the DNS records to point to your IP address if your public IP is changed by your ISP. I have my private domain pointing to a DuckDNS hostname.\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\n Configure the Reverse Proxy for Unifi Devices\n The config is relatively simple at this point. Check out the image below to see what the config should look like. Under \u0026ldquo;Domain Names,\u0026rdquo; add your domain (e.g., unifi.whitematter.tech). The \u0026ldquo;Scheme\u0026rdquo; should be \u0026ldquo;https\u0026rdquo; and the \u0026ldquo;Forward Hostname / IP\u0026rdquo; should be the local IP of your Unifi Dream Machine or other Unifi device (e.g., 192.168.1.1, 10.0.0.1). The \u0026ldquo;Forward Port\u0026rdquo; should be 443. I always check \u0026ldquo;Cache Assets.\u0026rdquo; Since Unifi devices are protected with Unifi account, I leave the \u0026ldquo;Access List\u0026rdquo; public. You can protect it with a username/password, but I recommend instead using a strong Unifi password, which is better in the long run anyway.\nNow, click \u0026ldquo;SSL\u0026rdquo; in the top menu (see the next image below). Choose \u0026ldquo;Request a new SSL Certificate\u0026rdquo; and check the \u0026ldquo;Force SSL,\u0026rdquo; \u0026ldquo;HTTP/2 Support,\u0026rdquo; and \u0026ldquo;HSTS Enabled\u0026rdquo; options. Enter an email address (this is used to send to Let\u0026rsquo;s Encrypt to generate your cert), and finally agree to the Let\u0026rsquo;s Encrypt terms after reading them. Click \u0026ldquo;Save,\u0026rdquo; and you\u0026rsquo;re good to go!\nClosing Assuming you followed these instructions successfully and your DNS records are correct, you should be able to go to unifi.yourdomain.com and connect to your Unifi device with the cert generated from Let\u0026rsquo;s Encrypt. You will notice that the annoying error will not appear upon connection! When you view the cert info associated with your Unifi domain now, you will be directed to https://letsencrypt.org/documents/isrg-cps-v4.1/ !\nThis solution is nice to me because you only have to remember your subdomain instead of an IP address for your devices. This also lets you connect to your device without going through Unifi\u0026rsquo;s cloud-based connection while outside of your local network.\nLike mentioned above, anytime you make services available to the world, you should protect your resources with strong passwords. I recommend a password manager to help with this. I use 1Password and a Yubikey.\n As always, if you have any questions, feel free to post below or reach out to me at robert@whitematter.tech.\nThanks for reading!\n ","permalink":"https://whitematter.tech/posts/how-to-connect-to-your-unifi-dream-machine-or-unvr-with-ssl-from-lets-encrypt/","summary":"Alright, if you have a Unifi device like a Dream Machine, Dream Machine Pro, UNVR, CloudKey, or other device, you likely have been met with the dreaded red triangle followed by the tedious words, \u0026ldquo;Your connection is not private.\u0026rdquo;\nFor some reason, at the time of this writing, there is still no official/built-in way to generate a cert for your device, though some interesting GitHub projects exist that claim to allow this functionality if you\u0026rsquo;re feeling lucky.","title":"How to Connect to Your Unifi Dream Machine or UNVR with SSL from Let's Encrypt"},{"content":"Reverse proxies are powerful tools used typically to forward client traffic to a server. In contrast to a forward proxy, a reverse proxy sits in front of web servers or other servers and forwards client traffic to the appropriate server. In this post, I will show you how to easily setup a reverse proxy using Docker, forward the necessary ports to the reverse proxy, and configure the reverse proxy to forward traffic to various servers on your network. Specifically, I will show how to setup the reverse proxy for se with WordPress, though the applications of this reverse proxy are endless!\nFor this tutorial, we will be setting up Nginx Reverse Proxy with Nginx Proxy Manager, which acts as a GUI frontend to manage your reverse proxy. Managing Nginx without a GUI is definitely doable, but I am not much of a fan of troubleshooting that setup. For this setup, I recommend a singular device or VM that can host your reverse proxy. You will need docker-compose installed on your system.\nFor this setup, I am using a Ubuntu bare-metal machine behind a Unifi Dream Machine Pro . You can use a VM or an OS on bare-metal capable of running Docker (for this tutorial though, we will use commands and terminology only applicable to Ubuntu, but you can make adjustments where necessary if you are familiar with Docker and choose not to use Ubuntu).\nFor tips on running a self-hosted WordPress site, which will be referenced in this post, check out this tutorial. You could run your WordPress site from the same machine/VM running your reverse proxy, but you will likely have to edit some docker-compose files. I may follow up with a post about that later, but until then, I suggest separate devices/VMs.\n Installing Nginx Proxy Manager\n To simplify your install, I have hosted the docker-compose.yml I use to deploy my Nginx Proxy Manager: GitHub. You can easily pull this docker-compose.yml with cURL or wget by running the commands below:\ncurl -LJO [https://raw.githubusercontent.com/robertomano24/nginx-proxy-manager/main/docker-compose.yml](https://raw.githubusercontent.com/robertomano24/nginx-proxy-manager/main/docker-compose.yml) OR\nwget --no-check-certificate --content-disposition [https://raw.githubusercontent.com/robertomano24/nginx-proxy-manager/main/docker-compose.yml](https://raw.githubusercontent.com/robertomano24/nginx-proxy-manager/main/docker-compose.yml) If you prefer to create your docker-compose.yml file yourself, make a file named \u0026ldquo;docker-compose.yml\u0026rdquo; and add the following to the file:\nversion: \u0026#39;3\u0026#39; services: app: image: \u0026#39;jc21/nginx-proxy-manager:latest\u0026#39; restart: unless-stopped ports: - \u0026#39;80:80\u0026#39; - \u0026#39;81:81\u0026#39; - \u0026#39;443:443\u0026#39; environment: DB_MYSQL_HOST: \u0026#34;db\u0026#34; DB_MYSQL_PORT: 3306 DB_MYSQL_USER: \u0026#34;npm\u0026#34; DB_MYSQL_PASSWORD: \u0026#34;npm\u0026#34; DB_MYSQL_NAME: \u0026#34;npm\u0026#34; volumes: - ./data:/data - ./letsencrypt:/etc/letsencrypt db: image: \u0026#39;jc21/mariadb-aria:latest\u0026#39; restart: unless-stopped environment: MYSQL_ROOT_PASSWORD: \u0026#39;npm\u0026#39; MYSQL_DATABASE: \u0026#39;npm\u0026#39; MYSQL_USER: \u0026#39;npm\u0026#39; MYSQL_PASSWORD: \u0026#39;npm\u0026#39; volumes: - ./data/mysql:/var/lib/mysql Once you have either downloaded the file or created your own, build your containers by issuing the following command:\ndocker-compose up -d If all goes well, you should now have Nginx Proxy Manager and MariaDB (for the reverse proxy database) running successfully.\n Configuring Nginx Proxy Manager\n Access your reverse proxy frontend by going to http://youripaddress:81 (e.g., http://192.168.1.10:81). Login with the default user email and password shown below. Immediately change the login email and password for your admin user to secure your reverse proxy.\nEmail: admin@example.com Password: changeme Once your password is changed, you can begin configuration. For an example, I will show you how I am configuring the reverse proxy for this WordPress site. You can fill in your own server information. This particular configuration assumes that you have forwarded your domain name (e.g., whitematter.tech) to your public IP. If you do not have a static public IP address, you might consider a free dynamic DNS service like DuckDNS, which will automatically update the DNS records to point to your IP address if your public IP is changed by your ISP. I have whitematter.tech pointing to a DuckDNS hostname.\nForward Ports 443 and 80 The first step is to forward ports 80 and 443 to your reverse proxy host (e.g., 192.168.1.10). I use a Unifi Dream Machine Pro (UDMP) as my gateway, so the screenshots will reflect the Unifi Controller. For Unifi users, go to Settings \u0026gt; Routing \u0026amp; Firewall \u0026gt; Port Forwarding. Create a new rule and name it something descriptive (I named mine NGINGX_443). Check to enable the forward, check the WAN interface, and check \u0026ldquo;Anywhere.\u0026rdquo; In both the Port and the Forward Port, enter \u0026ldquo;443.\u0026rdquo; Add your reverse proxy host local IP in the Forward IP spot (e.g., 192.168.1.10). Save the rule, and create another rule with the same info, but replace \u0026ldquo;443\u0026rdquo; with \u0026ldquo;80.\u0026rdquo; Be sure to save this rule too! You should then have two rules: NGINX_443 and NGINX_80. This will forward all incoming Internet traffic using ports 443 (HTTPS) and 80 (HTTP) to your reverse proxy server.\nConfigure a Proxy Host Go back to your Nginx Proxy Server at http://youripaddress:81. Select Hosts \u0026gt; Proxy Hosts. Near the top right, select Add Proxy Host. From here, we can add our first proxy host! Under the Details section, fill in the info similarly to the screenshot below. Under Domain Names, enter the URL and subdomains of the traffic that you want forwarded to your server. For example, if you have a webserver like I do hosting a WordPress site, you could inout your domain whitematter.tech and www.whitematter.tech. If you were hosting a Docker container elsewhere on your network, you might have something like homeassistant.whitematter.tech or something. This will certainly vary on your use-case. Feel free to reach out if you have more questions or need assistance with this piece! Under Scheme, I recommend https, but this also may vary. For a WordPress site, definitely select https. Under the Forward Hostname / IP, input your server IP as shown below. For the forward port, you will use the port required for traffic flow. If using a WordPress site, you will use 443. If using a Docker container like HomeAssistant, you would use something like 8123. Finally, check Cache Assets, Block Common Exploits, and Websockets Support.\nNext, select Custom locations and click Add location. If forwarding to a WordPress server, type \u0026ldquo;/wp-admin\u0026rdquo; as the location. The scheme should match the previous page, which is https in this case. Again, add your server IP and forward port like the previous page as well.\nFinally, under SSL, you can easily configure a LetsEncrypt certificate for use with your server, which will secure your connection to that server. You will probably not need this functionality every time you set up a proxy host especially if you use other means to generate SSL/TLS certs, but I will show you this function anyway! Under SSL Certificate, select \u0026ldquo;Request a new SSL Certificate\u0026rdquo;. Check Force SSL and HTTP/2 Support. Enter your email address at the bottom of the config and check to agree to LetsEncrypt\u0026rsquo;s ToS. Click Save. A cert will generate, and your proxy host will be configured!\nYour proxy host should look something like this:  Conclusion\n Once your host is added, you can add more following similar guidelines! The config may change slightly or substantially depending on your end goal, but the endless possibilities cannot be covered in a single post. However, I use this reverse proxy setup to add SSL for my connection to my Unifi Dream Machine Pro (UDMP). This entails forwarding ports 443 to the UDMP and adding the following Location: \u0026ldquo;/*\u0026rdquo;. This is one of many neat applications of this reverse proxy setup.\nPlease feel free to reach out to me if you have any questions or if this post was helpful! I love to hear how people are running their homelabs or offices. Comment below or shoot me an email to robert@whitematter.tech. Thanks for reading!\n\n","permalink":"https://whitematter.tech/posts/run-a-reverse-proxy-using-docker/","summary":"Reverse proxies are powerful tools used typically to forward client traffic to a server. In contrast to a forward proxy, a reverse proxy sits in front of web servers or other servers and forwards client traffic to the appropriate server. In this post, I will show you how to easily setup a reverse proxy using Docker, forward the necessary ports to the reverse proxy, and configure the reverse proxy to forward traffic to various servers on your network.","title":"How to Easily Run A Reverse Proxy using Docker"},{"content":"A Little Background and Some Thoughts\nWith today\u0026rsquo;s technology, we are constantly bombarded with High Energy Visible (HEV) blue light from our screens as well as inconsistent lighting throughout the day within our homes and workplaces. Like all mammalian species, we humans have our own circadian rhythms which occur naturally and are vitally important to our health and well-being. Rather than writing an essay on the importance of our natural circadian rhythms, check out the educational write-up from the NIH here. Hopefully this will help explain the \u0026ldquo;why\u0026rdquo; of this particular post.\nUse of Home Automation and open-source resources allows us to program our homes and lights to keep our natural rhythms aligned and consistent. This is an excellent use of technology to encourage a more natural environment, when technology often lends itself to the opposite effect (especially considering HEV blue light, etc.).\nI whole-heartedly recommend this described home automation to better your health and your family\u0026rsquo;s health, both mentally and physically. My wife and I are expecting our first child later this year. Evidence suggests that disruptions of the circadian rhythm of a prospective mother may have negative implications for her offspring after birth. Additionally, research suggests certain reasonable modifiable behaviors, such as simply going to sleep earlier, not only may positively impact a newborn child but also may protect against certain symptoms in the mother in the early postpartum period. There are hundreds of studies around circadian rhythms concerning the non-pregnant as well. In my years of undergraduate/post-bacc work in psychology and neuroscience as well as my graduate training in medical school, we studied the positive impact of natural physical inclinations to one\u0026rsquo;s overall physical and mental health. With the development and evolution of human life around a normal \u0026ldquo;sun-clock,\u0026rdquo; we often reap benefit from tuning ourselves to our natural state and environment around us. Circadian Lighting can help your body stay in tune with the sun, even if you work third shift. Now, let\u0026rsquo;s talk about how you can get this automation started for you and your family.\nGetting Started This automation relies specifically on Home Assistant, a home automation platform designed to interact with your various smart-home devices from a single web-panel. You will need some smart lights (I use and specifically recommend Philips Hue with a Hue Bridge, though most other smart lights will probably work), a device to host Home Assistant (like a Raspberry Pi, a Docker container, a NAS, or a PC of some sort), and finally, a little patience for the setup!\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nInstalling Home Assistant\nThere are many excellent tutorials on the web for installing/setting up Home Assistant. I will not cover this, but I will refer you instead to the official Home Assistant Documentation.\nConfiguring Home Assistant and Installing Circadian Lighting\nFirst, download Circadian Lighting. There are two options for this. Option 1 is recommended if you use Phillips Hue lights and have a Hue bridge. Option 2 is recommended if you do not use Phillips Hue lights. For whichever option you choose, hit the link and click the green \u0026ldquo;Code\u0026rdquo; button near the top right of the screen. Choose to \u0026ldquo;Download Zip.\u0026rdquo; Once the Zip is downloaded, use your favorite archive utility to extract the contents of the Zip.\nYou then must make a folder within your Home Assistant config root folder called \u0026ldquo;custom_components\u0026rdquo; (without the quotes). See the image below, as this folder should be in the same directory as the configuration.yaml. Inside the custom_components folder, make a folder called \u0026ldquo;circadian_lighting\u0026rdquo; (without the quotes). Inside that folder, place the contents of the Zip (I.e., ___init___.py, manifest.json, sensor.py, services.yaml, and switch.py). The structure should look like this: config/custom_components/circadian_lighting/\nFolder Structure Setup\nNext, we will need to modify the configuration.yaml. We need to add the following lines to the file:\n# Example configuration.yaml entry circadian_lighting: Next, you will need to create a switch in Home Assistant, which will act as a controller to turn of Circadian Lighting if you want to use your color lights or change the brightness at some point. Here is an example:\n# Example configuration.yaml entry switch: - platform: circadian_lighting lights_ct: - light.desk - light.lamp If you want, you can create multiple switches so you can turn Circadian Lighting off in one, two, or more rooms independently from the rest of your home. You must add your lights manually to each switch by adding the entity ID from Home Assistant. You can see my personal example of multiple switches on my Home Assistant GitHub repo.\nHue Specific Setup (Option 1)\nThe main difference between Option 1 and Option 2 mentioned above is that Option 1 allows you to have scenes set per room that can be assigned to a Hue switch, which lets you turn on your lights to the correct Circadian Lighting value. In each room where you would like to turn on the Circadian Light scene with a Hue switch/dimmer, you will need a scene in that room called \u0026ldquo;Circadian\u0026rdquo; without quotes. The \u0026ldquo;C\u0026rdquo; must be capitalized. I use the iConnectHue app on my iPhone to make these scenes. Different apps may not appropriately make the scenes, and I recommend iConnectHue anyway. In a room, like Kitchen, add a scene with the bulbs you would like to be turned on with your switch. The color/brightness values do not matter as this plugin will overwrite them. Create the Circadian scene and assign the scene to your switch (see the images below for screenshots of my iConnectHue app setup).\nKitchen Lights Example\nCreated Scene Example\nThere you have it! Once the scene is setup (for Option 1, only), your setup is complete! Option 2 will be complete too! The last thing to do is restart your Home Assistant instance, and your Circadian Lights will be keeping your rhythm in sync with nature! For advanced setup ideas, check out the \u0026ldquo;Issues\u0026rdquo; on GitHub.\nWrapping Up\nI hope this post was helpful getting your Circadian Lighting plugin working. I truly believe it will aid the physical and mental health of you and your family. If you have any issues, questions, or something was not clear in this post, please comment below, email me at robert@whitematter.tech, or make an \u0026ldquo;Issue\u0026rdquo; on GitHub. Since you have IoT devices on your network, be sure to check out the post on securing your network with VLANs\nThanks for reading!\nNote: For your first launch with the plugin installed, you may need to turn on the Switch(s) manually. On your Home Assistant Dashboard, you can add entities to your home page like the image below. Once added, you can flip the switch to enable Circadian Lighting for those lights!\nCL Switches on Home Assistant Dashboard\n","permalink":"https://whitematter.tech/posts/live-better-with-circadian-lighting/","summary":"A Little Background and Some Thoughts\nWith today\u0026rsquo;s technology, we are constantly bombarded with High Energy Visible (HEV) blue light from our screens as well as inconsistent lighting throughout the day within our homes and workplaces. Like all mammalian species, we humans have our own circadian rhythms which occur naturally and are vitally important to our health and well-being. Rather than writing an essay on the importance of our natural circadian rhythms, check out the educational write-up from the NIH here.","title":"Live Better with Circadian Lighting"},{"content":"IoT Overview The smart world of Internet-of-Things (IoT) devices is ever growing. From everyday lightbulbs to the sprinkler out front, just about every household appliance and utility has a smart-counterpart. For example, my smart home is fully Apple HomeKit compatible and consists of a Hue bridge with lightbulbs, Lutron Caseta smart dimmers/switches, Eve Aqua outdoor water hose control, iSmartGate garage door opener, Schlage deadbolt, Eve motion sensor, Sonos speakers throughout the house, a Vocolinc oil diffuser, Vocolionc power strip, a couple iRobot Roomba vacuum cleaners, some Vocolinc pluggable outlets, an Ecobee thermostat to replace each analog thermostat in the house, and a Unifi G4 Doorbell (the doorbell is not technically compatible with HomeKit, but I added support with a third-party tool known as \u0026ldquo;Homebridge\u0026rdquo;). On top of all these smart home devices, I have a handful of other Unifi Protect cameras around my property.\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nIf you\u0026rsquo;re reading this, you may not have quite that many devices, but you are probably either looking to invest in some smart home devices or you have already started the process of converting your home to the world of IoT. Either way, it is important to consider the security implications of adding these devices to your network. For example, some smart switches have such poor encryption that they are easily compromised and can be overpowered to catch on fire. While none of the devices in my home have this sort of vulnerability that has been publicized, the possibility exists nonetheless that these devices could be used by nefarious actors to cause ruckus in my home or attempt to gain access to other devices on my network.\nIoT devices continue to improve their security mechanisms (mostly), and IoT is certainly not going away anytime soon. Because of this, prudent users, like you, should consider how to best protect their internal resources. One recommended method of securing your network containing IoT devices is to segment your network with VLANs. I will show you how to segment your home network from your IoT devices with VLANs, including how to create subnets, VLANs, firewall rules, and how to enable IPS/IDS for good measure. To follow along, your network will need to be comprised of Unifi networking gear. My gateway and Unifi controller is the Unifi Dream Machine Pro, though you could use any Unifi gateway + controller combination. Additionally, many other network providers besides Unifi will have similar functionality, and you will likely be able to accomplish some of these tasks with other gear.\nCheck out a community ports list for IoT on GitHub here: https://github.white.fm\nCreating VLANs and Segmenting the Network The first step is creating a VLAN for your IoT network. The easiest way to accomplish this task is to create a new subnet for your IoT network. For example, if you currently just have a single network for all of your Internet-connected devices, including your personal computers, phones, etc, you probably have a single subnet that looks something like this: 192.168.1.1/24. This is seen on your Unifi Controller by going to Settings \u0026gt; Networks (on the old GUI). I recommend you next click \u0026ldquo;Create New Network,\u0026rdquo; and name the network something like \u0026ldquo;IoT\u0026rdquo;. Specifically select \u0026ldquo;Corporate\u0026rdquo; for the \u0026ldquo;Purpose.\u0026rdquo; It makes it easy to remember if you set the Gateway IP/Subnet 1 number off from your default network (e.g., set it to something like 192.168.2.1/24 or 192.168.10.1/24). Below, you will see my settings. Notice my Gateway is 10.100.1.1. My default subnet is 10.100.0.1, in contrast. For simplicity, I have IGMP snooping and UPNP enabled. This might help down the road for certain smart components like Home Assistant. For VLAN, set any number from 2-4018. I set my DHCP range to only include x.101-x.254 because I wanted to reserve the first 100 IPs in this subnet for static addressing. If you want all your devices to be DHCP, you do not need to modify this option. Go ahead and save this network.\nPreparing the Wireless\nSince most IoT devices are wireless, you will need to create a wireless SSID for all of your IoT devices to connect to separately from the rest of your home network. Go to Settings -\u0026gt; Wireless Networks, and click \u0026ldquo;Create New Wireless Network.\u0026rdquo; Name the SSID something memorable, and set the security to \u0026ldquo;WPA Personal\u0026rdquo; (old GUI\u0026hellip;the new GUI may allow you to specifically choose WPA2, in which case, do that). For \u0026ldquo;Network,\u0026rdquo; choose the subnet/VLAN you made in the previous step. This will associate the new SSID to that particular network segment.\nContinue through the setup screen to \u0026ldquo;Advanced Options.\u0026rdquo; Here, I specifically have ONLY 2.4GHz enabled for WiFi Bands because most IoT devices still only use 2.4GHz. If you happen to have some 5GHz IoT devices, the slightly better performance for those devices will likely be overshadowed by the constant disconnecting of your devices when both 2.4GHz and 5GHz are enabled simultaneously. IoT devices in general do a pretty bad job of handling Unifi APs with both bands enabled. Feel free to try enabling both bands in your environment, but if you have lots of issues with connectivity and the infamous \u0026ldquo;No Response\u0026rdquo; message on Apple HomeKit, I recommend again to stick with just 2.4GHz for now. Disable all of Unifi\u0026rsquo;s \u0026ldquo;BETA\u0026rdquo; tagged options like Fast Roaming, as these will also cause performance and connectivity issues. I also prevent my SSID from being broadcast to clean up the WiFi experience for users at home. It is a lot nicer to only see my LAN and my guest networks being broadcast. This will, however, force an extra step when you try to add devices to HomeKit, as you will need to connect your phone to the IoT WiFi before adding the IoT devices to that network. Manually adding the SSID can be tedious, but it is well worth it in my opinion once the overall setup is complete. I leave GTK rekeying to 3600 seconds. I also have a User Group setup for IoT devices in case I want to throttle IoT in the future. Check to Enable multicast enhancement (IGMPv3). Leave everything else as is, and you\u0026rsquo;re finished with the Wireless setup.\nBlocking Traffic Between Subnets/VLANs The next part of this process will be setting up the Firewall to block traffic between the subnets/VLANs. Go to Settings \u0026gt; Routing \u0026amp; Firewall \u0026gt; Firewall. I will assume you are only using IPv4, and we will therefore only look at IPv4 rules. For a detailed definition of the WAN IN, WAN OUT, WAN LOCAL, etc. options, I will recommend you search the Internet. This part can be pretty confusing, and the definition of these options is outside the scope of this post. We will focus on LAN IN and LAN LOCAL for our purposes.\nLAN IN\nThis portion can get complicated depending on how specific you want to be with allowances of devices between these network segments. I have a lot of different home automation programs running like HomeBridge and Home Assistant. I also have Sonos speakers, which need their own rules to function properly with your iPhone on a different subnet. If you have some IoT devices (no Sonos) without any external programs like HomeBridge, the only rules you will need to concern yourself with are 2003 and 2012.\nLet\u0026rsquo;s look at the mentioned Rule 2003. Go ahead and create a rule in the LAN IN section. In the image below, you can see my settings for this rule. Essentially, this rule allows your devices in your default network to communicate with your IoT devices only (traffic flow LAN -\u0026gt; IoT). Match your settings to the settings below. Be sure Action is set to Accept.\nNext, let\u0026rsquo;s check out Rule 2012. Create a new rule and match to the image below. Note that Action Is set to Drop. Also, note the States that are checked are New and Invalid. This prevents traffic from flowing from the Source (IoT) to Destination (LAN) (traffic flow IoT -\u0026gt;X LAN) based on the status of the traffic. Only established and related traffic will be allowed, then.\n**\nLAN LOCAL**\nNext, let\u0026rsquo;s check LAN LOCAL rules These rule is concerned with allowing multicast DNS traffic and with allowing HomeKit-specific ports to receive data as needed. I have a Port Group with ports 51826 and 51827 for HomeKit. Make your own rules and match your settings to the image below. For the sake of space, I will show one image. All you will need to do is change your source/destination like in the image above (Rules 2000-2003). You are targeting specific ports. One rule will allow ANY:5353 -\u0026gt; ANY:ANY, one will allow ANY:ANY -\u0026gt; ANY:5353, one will allow ANY:51826-7 -\u0026gt; ANY:ANY, and finally one will allow ANY:ANY -\u0026gt; ANY:51826-7. Make and save your rules!\nWrapping Up There you have it! You now have a fully-functional HomeKit setup enabled with extra security practices to prevent mischief from poorly-secured IoT devices reaching your internal LAN. It is clear this does not mitigate 100% of the risk since we\u0026rsquo;re allowing traffic to flow in opposite direction. You can lock your subnets down even more by experimenting with fully blocking your traffic from your LAN to your IoT network but ONLY allowing instead your HomeKit controller (e.g., Apple TV, Homepod, etc.). These rules can and probably should be tweaked to fit your environment, but the rules described above will at least get you started. If you have any specific issues or have an IoT device not functioning properly, please reach out in the comments or shoot me an email!\n","permalink":"https://whitematter.tech/posts/how-to-add-vlan-segmentation-for-homekit-iot-devices-with-unifi/","summary":"IoT Overview The smart world of Internet-of-Things (IoT) devices is ever growing. From everyday lightbulbs to the sprinkler out front, just about every household appliance and utility has a smart-counterpart. For example, my smart home is fully Apple HomeKit compatible and consists of a Hue bridge with lightbulbs, Lutron Caseta smart dimmers/switches, Eve Aqua outdoor water hose control, iSmartGate garage door opener, Schlage deadbolt, Eve motion sensor, Sonos speakers throughout the house, a Vocolinc oil diffuser, Vocolionc power strip, a couple iRobot Roomba vacuum cleaners, some Vocolinc pluggable outlets, an Ecobee thermostat to replace each analog thermostat in the house, and a Unifi G4 Doorbell (the doorbell is not technically compatible with HomeKit, but I added support with a third-party tool known as \u0026ldquo;Homebridge\u0026rdquo;).","title":"How to Add VLAN Segmentation for HomeKit IoT Devices with Unifi"},{"content":"Overview The purpose of this post is to provide some tips to address some network security concerns when hosting an externally-facing web server from a device within your home network. For this post, I will be using Unifi networking gear. My screenshots will be of the Unifi controller on my Unifi Dream Machine Pro (UDMP), but I will do my best to overview the concepts so you can replicate with your own networking gear. Let\u0026rsquo;s get started!\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nStrong Passwords/Encryption Keys Here is your obligatory \u0026ldquo;strong password\u0026rdquo; spiel. Be sure, during the setup and maintenance of any services, OSs, logins, databases, etc., that you use complex and variable passwords and encryption keys where possible. Utilizing a password manager (I recommend 1password.com) can help you in this endeavor by allowing you to easily generate and securely store complex passwords/encryption keys. This step alone will help mitigate security concerns overall even if a hacker were able to compromise your network security mechanisms. If it takes a hacker one-million years to brute-force the encryption key for your WordPress database, gaining access to the network in which the database rests approaches pointless. This idea is true regarding your OS root user(s) passwords, GUI application logins/passwords, databases encryption keys, and anything that is secured with a password or encryption key. The concept here is to do your best to prevent hackers from gaining access to your network in the first place (I will show you some tools at your disposal if you use Unifi networking gear in this post), but if they somehow manage to succeed gaining access, have your resources utilizing passwords/encryption keys locked as much as possible with long, complex, non-reused strings.\nBuild and Configure an Isolated \u0026ldquo;DMZ\u0026rdquo;-like Network DMZ stands for \u0026ldquo;de-militarized zone.\u0026rdquo; DMZs in practice are relatively antiquated in today\u0026rsquo;s networking world, as the concept behind a DMZ is to clump a bunch of relatively insecure servers/resources together and hope a hacker is unable to pivot to your internal network. You will still hear the terminology, especially if you explore technical certifications like CompTIA\u0026rsquo;s Security+. The concept behind what we will do is relatively similar to the DMZ. We will setup VLANs and firewall rules on our router to prevent any traffic from coming from the \u0026ldquo;DMZ\u0026rdquo; network to the internal network(s). From this point forward, I will refer to the subnet we will create containing the WebHost as the \u0026ldquo;DMZ network,\u0026rdquo; and all other subnets will be referred to as \u0026ldquo;internal networks.\u0026rdquo;\nNote: If you are following along with my images, you will need to ensure you are on the old settings page. To disable the new interface, go to Settings \u0026gt; System Settings and locate the \u0026ldquo;New User Interface\u0026rdquo; check box. Uncheck and click \u0026ldquo;Deactivate\u0026rdquo; on the popup to go back to the old settings for now. I will try to update this post in the future with the new interface.\nTo get started, in Unifi, go to Settings \u0026gt; Networks, and add a Corporate Lan and name it whatever\u0026ndash;I named mine DMZ. For other networking gear, you will want to create a subnet or isolated iteration of a LAN or VLAN, but it helps if you are able to make a new subnet. On my Corporate LAN on Unifi, I assigned VLAN 777 to this subnet.\nNow, we can assign any device to this network. Below is a screenshot of the port assignment for my webhost. You can assign any physical port on your Unifi switches to this particular subnet. Other networking devices will probably have similar setups or allowances. If your particular networking equipment does not, perhaps it is time to consider switching to Unifi!\nSetting Firewall Rules\nNext, go to Settings \u0026gt; Routing \u0026amp; Firewall \u0026gt; Firewall. From here, we want to select LAN IN. Click \u0026ldquo;Create New Rule.\u0026rdquo; I recommend naming your rules something descriptive and helpful (e.g., Block DMZ -\u0026gt; IoT). You will want to make sure the rule is Enabled and \u0026ldquo;Before predefined rules\u0026rdquo; is selected for the Rule Applied option. Check \u0026ldquo;Drop\u0026rdquo; as the Action, and choose \u0026ldquo;All\u0026rdquo; for the IPv4 Protocol selection. Now, under Advanced, select all available states (i.e., New, Established, Invalid, Related). Leave the IPsec option as is. Finally, we will change the Source and Destination to match our intended subnets. Under Source, choose \u0026ldquo;Network\u0026rdquo; for \u0026ldquo;Source Type\u0026rdquo; and select your DMZ network. Similarly, for Destination, choose \u0026ldquo;Network\u0026rdquo; for \u0026ldquo;Source Type\u0026rdquo; and select your internal network (in the image below, I chose my IoT network). This will drop all IPv4 traffic in any state from your DMZ network destined for your internal network. Be aware that this will cause trouble if you need to access your WebHost for some reason from your internal networks. This will break SSH, RDP, etc. See below if you need to setup certain protocol access to your WebHost.\nIf you have multiple internal networks defined as Corporate LANs in Unifi, you will need to recreate the above rules for each network. In the image below, you can see I set up this same rule for my IoT and Protect networks.\nAllowing Specific Traffic From DMZ to Internal Network Any adjustments made at this point will lead to unnecessary security concerns. I highly recommend avoiding making these adjustments unless absolutely necessary. Before making the changes, ask yourself if there is any possible other way to achieve your goal\u0026ndash;protect your internal resources as much as possible.\nAdding Firewall Rules\nThe image below shows how your configuration might look if you need certain protocols open between your WebHost and your internal network. Notice the changes from our previous rules: Action = Accept, Invalid = Unchecked, Source = Address/Port Group.\nConcerning Source specifically, I highly recommend specifying which WebHost will need this access. For example, if you have multiple WebHosts, and you would like your WordPress WebHost to be accessible via SSH, choose \u0026ldquo;Create IPv4 Address Group.\u0026rdquo; Input a descriptive name (e.g., WebHost), and add the static IP of the WebHost under \u0026ldquo;Address\u0026rdquo; (e.g., 10.99.100.77).\nNote: Your WebHost really should have a static address in general, so if you do not currently have it configured that way, go back and give the WebHost a static address before proceeding.\nFinally, continuing under Source, select \u0026ldquo;Create Port Group.\u0026rdquo; Here, add the ports which you would like to be able to access from your internal network. Below, you can see I added SSH (port 22) and RDP (port 3389). Click \u0026ldquo;Save.\u0026rdquo; Under Destination, you can either specify which networks/specific devices need this access or allow any of your internal networks to access these ports by leaving \u0026ldquo;Any\u0026rdquo; selected in both boxes.\nEnable IPS/IDS The Unifi Dream Machine Pro comes with some excellent security features. The feature we will look at next is IPS/IDS located under Settings \u0026gt; Threat Management (old GUI) or Settings \u0026gt; Security \u0026gt; Internet Threat Management (new GUI). IPS stands for Intrusion Prevention System while IDS stands for Intrusion Detection System. The best way to think about these concepts is to think about IPS as an \u0026ldquo;active\u0026rdquo; system while IDS is a \u0026ldquo;passive\u0026rdquo; system. IDS pays attention to all of your traffic and logs/alerts you if something nefarious/abnormal is detected. IPS on the other hand pays attention to all of your traffic and actively prevents/blocks traffic it detects as nefarious/abnormal based on the rules you give it.\n IPS may be the single best security measure you have at your disposal with the UDMP to protect your network while WebHosting at home.\n If you do not use Unifi, you may be able to get hardware or software that provides this service, but I am not knowledgeable about current consumer/prosumer-priced alternatives.\nIn your Settings \u0026gt; Threat Management page, I recommend choosing IPS for your Protection Mode. If you are not using a Unifi Dream Machine Pro, this may negatively impact your overall download speed. Before you enable IPS, your device will tell you what your maximum throughput will be with IPS enabled. The Unifi Dream Machine Pro is good to use IPS up to 3.5Gbps of throughput. Below you can see the way I currently have the IPS rules configured. The rules are similar in both version of the GUI (new and old). This image is from the old GUI, but you should easily be able to mix+match on the new GUI too.\nWrapping Up  Network security is an important topic to discuss in general, but the benefits of taking security seriously when WebHosting on a device at home cannot be overstated.\n I implore you to consider implementing the above outlined security measures regardless of your WebHosting status, but it should really not even be an option if you do indeed have a WebHost. Go ahead and get your isolated network prepared and enable IPS on your Unifi Dream Machine Pro or other Unifi Gateway device.\n","permalink":"https://whitematter.tech/posts/network-hardening-webhosting/","summary":"Overview The purpose of this post is to provide some tips to address some network security concerns when hosting an externally-facing web server from a device within your home network. For this post, I will be using Unifi networking gear. My screenshots will be of the Unifi controller on my Unifi Dream Machine Pro (UDMP), but I will do my best to overview the concepts so you can replicate with your own networking gear.","title":"How to Harden Your Network Security for Your In-Home Web Hosting"},{"content":"This post aims to show you how to use pfSense within a Unifi network behind a Unifi Gateway [in my case, the gateway is the Unifi Dream Machine Pro (hereafter referred to as UDMP)]. I will explain my current network configuration including applicable subnets, VLANs, and wireless SSIDs needed to make this setup successful. The end goal is to be able to add a client on my Unifi network to a particular VLAN either by joining this client wirelessly to a particular SSID or by tagging the client\u0026rsquo;s physical port to that VLAN. This VLAN will be tied to a subnet that sends data through the pfSense machine which is acting as a VPN client (I use Private Internet Access). This method allows the UDMP to continue to act as the DHCP server for these clients while allowing pfSense to anonymize and encrypt the data of the clients in question.\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nThis post assumes that you have the following: a Unifi Gateway device (e.g., UDMP, Unifi Security Gateway, etc.), a pfSense machine/VM, Unifi wireless APs (only if you want to add wireless devices to the VPN), and Unifi switches (only if you want to tag specific switch ports to the VPN). This post also assumes you have access to or a subscription to a VPN service. In this post, all references to VPN use will be specific to PIA. This guide may or may not work with other VPN providers initially. However, I am confident that, if you can initialize the client connection to your VPN provider from pfSense, you will be able to successfully use the tutorial to anonymous traffic with Unifi VLANs. We will first look at the pfSense setup and VPN configuration. After, we will explore the Unifi setup and configuration.\npfSense Setup and Configuration I built a custom pfSense machine with the following components: Intel(R) Core(TM) i5-8500 CPU @ 3.00GHz, GIGABYTE B365M DS3H, Corsair Vengeance LPX 16GB, Thermaltake Smart 500W Power Supply, and a 4-Port PCI-E Network Interface Card.\nSelect a PIA Server First, we need to select a server that works best. This likely will be mainly determined by your country and geographical area. With your acount username and password for PIA, you will be able to see a complete list of servers here: https://www.privateinternetaccess.com/pages/ovpn-config-generator\nTo import the certificate needed, choose the 1198 port option, and click \u0026ldquo;Generate\u0026rdquo;.\nOnce the file is downloaded, open it in your favorite text editor (e.g., Atom, Notepad++, etc.). Copy the portion -\u0026mdash;-BEGIN CERTIFICATE\u0026mdash;\u0026ndash; all the way through -\u0026mdash;-END CERTIFICATE\u0026mdash;\u0026ndash; as shown in the image below.\nCreate a Certificate Authority in pfSense In pfSense, navigate to System \u0026gt; Cert Manager and click on the \u0026ldquo;+ ADD\u0026rdquo; Button. Change the \u0026ldquo;Method\u0026rdquo; to \u0026ldquo;Import an existing certificate authority\u0026rdquo; and paste the copied certificate text into the box. It should look like below:\nClick Save.\nYou should now see the certificate listed:\nConfigure OpenVPN Client Now we have the certificate listed, navigate to VPN \u0026gt; OpenVPN, then click Clients and finally click ADD.\nSee the following images about changes to make in. your configuration. It should match mine with replacement of the Server Host and your PIA Username and Password. You will find the server host in the .ovpn file you downloaded earlier from PIA. Here is the info needed to copy+paste into the \u0026ldquo;Custom options\u0026rdquo; box toward the end of the configuration page:\nremote-cert-tls server\npfSense Gateway and Interface Assignment Now, we move to the complicated part. For this step, we will need to create a Gateway on pfSense for the traffic to use. I will detail this more later, but my setup contains a WAN (corresponding to a 10.99.1.0/24 subnet on Unifi), LAN (corresponding to a 10.99.2.0/24 subnet on Unifi), VLAN500 (which is the VLAN tag on my Unifi setup) and PIA interface. I will detail the setup of each, which will move us to the Unifi setup.\nConfigure the Gateway\nFirst, go to System \u0026gt; Routing \u0026gt; Gateways and click \u0026quot;+ Add\u0026quot; to add a Gateway for this setup. Add the Gateway IP from your Unifi Gateway, which, in my case, the Gateway IP is 10.99.1.1 corresponding to the Unifi UDMP\u0026rsquo;s address in that subnet (subnet is 10.99.1.0/24, named pfSense WAN on Unifi). Save and continue.\nInterface Assignments\nNow go to Interfaces \u0026gt; Assignments. If a WAN interface is already assigned, great! Edit to ensure it looks similar to below (I chose 10.99.1.15 for my static WAN IP because it was easy to remember. You can use any address in the 10.99.1.0/24 subnet EXCEPT for the Gateway, 10.99.1.1). Be sure the IPv4 Upstream gateway is set to the Gateway we just made. Additionally, check \u0026ldquo;Block bogon networks\u0026rdquo; at the bottom of the configuration page.\nLikewise, check your existing LAN interface, if it exists, or create one as follows below. The LAN interface corresponds to the 10.99.2.0/24 subnet, where devices tagged with the VLAN 500 will go and retrieve their DHCP lease on Unifi. Again, I set my LAN interface IP to 10.99.2.15 because it seemed easy to remember for me. Make sure the IPv4 Upstream gateway is set to \u0026ldquo;None.\u0026rdquo; DO NOT block bogon networks on the LAN interface.\nAdd a PIA Interface\nOn the dropdown for \u0026ldquo;Available Network Ports\u0026rdquo; you should see your PIA VPN listed. Select your PIA VPN, and click add.\nNow that the interface has been added, click on the interface name (it will likely be something like OPT2 or OPT3, but yours may be different\u0026hellip;it should be at the bottom of the list). Now you can check the box to enable the interface and give it a better name, I called mine PIA_Netherlands. Also, make sure the reserved network checkboxes are unchecked.\nFinally, go to Interfaces \u0026gt; VLANs and add a VLAN. Here, choose your previously configured LAN interface as the parent interface, and choose a VLAN tag between 1 and 4094. This tag will be used in Unifi. I chose 500.\nFinally, you will need to plug in both physical interfaces corresponding to your WAN and LAN interfaces. If you have only two NICs on your pfSense box, you should easily be able to figure out which port corresponds to which interface. If you have more than two NICs, it may be more difficult. You need to know which port is which because you will need to tag the LAN interface with the VLAN 500 in Unifi (you can do this now, but we will cover it shortly).\nOutbound NAT Rules Select Firewall \u0026gt; NAT, and click Outbound.\nClick the radio button to change the outbound NAT mode to Hybrid, and click Save.\nYou will need to make rules for the traffic that will need to reach the VPN, which will be the subnet from Unifi that you will add (e.g., in my case, 10.99.2.0/24 is the PIA Subnet). The rules are as follows (see the image below): Localhost to PIA rule, ISAKMP Localhost to PIA rule, LAN (Subnet) to PIA rule, ISAKMP LAN (Subnet) to PIA rule.\nOutbound NAT Rules\nBelow, I will show the settings page for each of the rules that I needed. If you have more interfaces or subnets, you will need to create your own rules to match. Just replace my values with your own.\nYour \u0026ldquo;localhost\u0026rdquo; and \u0026ldquo;127.0.0.0\u0026rdquo; information will be the same as mine. You will just need to replace the PIA_CHICAGO with your own PIA Interface you created earlier along with your intended subnet to be created in Unifi (e.g., 10.99.2.0/24).\nISAKMP - localhost to WAN\nISAKMP - localhost to WAN\nlocalhost to WAN\nlocalhost to WAN\nISAKMP - LAN (Subnet) to WAN\nISAKMP - LAN (Subnet) to WAN\nLAN to WAN\nLAN to WAN\n This concludes our setup of pfSense.\nWe can now head over to Unifi!\n Unifi Setup and Configuration In our Unifi Settings page, we will create two Corporate LAN networks: one will be for the pfSense WAN, and the other will be for the pfSense LAN. As shown in the image below, the subnets correspond to the subnets we mentioned and configured for pfSense above.\npfSense WAN on Unifi\nMake a new Corporate LAN on Unifi for your pfSense WAN, and match the options to the options in the image below. Make sure your subnet matches the subnet added to pfSense earlier for the WAN interface.\npfSense LAN on Unifi\nMake another Corporate LAN in Unifi. Below is my settings page for the pfSense LAN. You can match your settings to mine. Be sure to set the DHCP Gateway IP to the static IP address you gave to the pfSense LAN Interface earlier (e.g., 10.99.2.15). Also be sure to add THE SAME VLAN as you did in pfSense earlier (e.g., VLAN 500).\nFinally for the Unifi LAN, you will need your pfSense physical port (the one corresponding to the LAN interface) plugged into Unifi, and you will need to manually set the port to use the VLAN you set up.\nTagging Ports on Unifi Switches\nFrom your Unifi clients page, select a client you want to add to pfSense. Under the overview of the client, select the Port it is plugged into (e.g., Family Room - UniFi Switch 8 POE-60W #7 as shown below).\nThis should bring up a page that allows you to specifically override the configuration of that port. At this point, select the new pfSense (500) Port Profile (if the Port Profile is not here, please skip to the end of the article to find out how to add the profile). Once you apply the configuration, your device will obtain a new DHCP lease from Unifi in the new pfSense subnet. Then, your wired device will be sending its traffic through PIA.\nAdding Wireless Clients to pfSense on Unifi APs\nGo to Settings \u0026gt; Wireless Networks and click \u0026ldquo;CREATE NEW WIRELESS NETWORK.\u0026rdquo; You can mimic the setup from the image below, but the important aspect here is to select pfSense as the Network with which the WLAN is associated (see pic below). Now, any devices connected to this Wireless network will have their traffic sent through PIA! Verify this is working by going to \u0026ldquo;whatismyip.com\u0026rdquo; on a mobile device or some other device on this WLAN.\n END\n Congratulations if you made it this far and everything is working!\nI hope this tutorial aids you in your endeavors to anonymize certain portions of your network. If you have any questions or need assistance, please let me know in the comments!\nTROUBLESHOOTING Adding Missing Port Profiles\nIf for some reason the Port Profile was not made automatically when you created your respective networks, you may need to go to Settings \u0026gt; Profiles in the Unifi dashboard. On this page, switch from RADIUS to SWITCH PORTS. Click \u0026ldquo;ADD NEW PORT PROFILE.\u0026rdquo; Name the Profile and then click Native Network, and assign the pfSense network to this profile as shown below. Once selected, click Save/Apply. You can return to the assignment area and assign ports appropriately to use the pfSense VPN.\n","permalink":"https://whitematter.tech/posts/how-to-use-pfsense-and-unifi-to-anonymize-and-encrypt-vlan-tagged-traffic/","summary":"This post aims to show you how to use pfSense within a Unifi network behind a Unifi Gateway [in my case, the gateway is the Unifi Dream Machine Pro (hereafter referred to as UDMP)]. I will explain my current network configuration including applicable subnets, VLANs, and wireless SSIDs needed to make this setup successful. The end goal is to be able to add a client on my Unifi network to a particular VLAN either by joining this client wirelessly to a particular SSID or by tagging the client\u0026rsquo;s physical port to that VLAN.","title":"How to Use pfSense and Unifi to Anonymize and Encrypt VLAN Tagged Traffic"},{"content":"My first post will, appropriately, show you how to build your own self-hosted Wordpress site utilizing Docker (just like this site)! For this setup, I am using a Ubuntu bare-metal machine behind a Unifi Dream Machine Pro . You can use a VPS or an OS on bare-metal capable of running Docker (for this tutorial though, we will use tools only applicable to Ubuntu, but you can make adjustments where necessary if you are familiar with Docker and choose not to use Ubuntu). Check out this project\u0026rsquo;s GitHub page for examples and help.\nAs an Amazon Associate, I earn from qualifying purchases. Thank you for supporting the maintenance of this blog. The pricing will be the same for you regardless if you use my links or not! Thanks for your support!\nThis post assumes you have Ubuntu installed with access to a user with root privileges, Docker and Docker-Compose installed, a domain that you own (if you do not have one, I suggest purchasing through Google Domains or Hover), the ability to modify your domain\u0026rsquo;s DNS records (I recommend CloudFlare), and the ability to forward ports on your network (unless you are using a VPS or other externally hosted OS).\nOne reason I am using Ubuntu to host this Docker setup is because of the ability to use Ubuntu\u0026rsquo;s environment file to store pertinent and sensitive variables securely. This allows me to easily share my docker-compose files without exposing my secrets in GitHub or elsewhere. There may be better alternatives to storing your variables with Docker\u0026rsquo;s built-in secrets management, but this tutorial will not cover that. Finally, consider the security implications of hosting a website on your network. Check out this post to get some ideas and walkthroughs of some easy things you can do to increase your network security for this endeavor. You might also consider checking out this post about reverse proxies if you run more than one webserver you need externally accessible.\nPrepare Your Domain For your domain, you will need to add DNS records pointing your domain, for example, whitematter.tech, to the public IP address of your server (if you are using a bare-metal install on your home network, this would be the public IP your ISP provides you, which can be determined at whatismyip.com). I personally prefer a Dynamic DNS solution like duckdns.org, which allows me to automatically keep updated with my changing public IP since I do not have a static address from my ISP (for tips on exploring duckdns for your setup, check out this video on YouTube). The image below shows what CloudFlare looks like for my domain being pointed to my duckdns address (if you do not have duckdns, \u0026ldquo;Content\u0026rdquo; would be your public-facing IP, like 177.99.88.10).\nSet Environment Variables On Ubuntu, open the Environment file by issuing the following command:\nsudo nano /etc/environment Copy and paste the following underneath the PATH in the file. Replace the generic values in red below with your own. DO NOT modify the PATH itself for your configuration (i.e., PATH=\u0026quot;\u0026quot; should not be changed!).\nTo find your time zone (TZ), see this list.\nReplace “username” in the USERDIR with your Ubuntu username. I strongly recommend using a password manager (I use 1Password) to generate and store passwords for MySQL and WP databases.\nPATH=\u0026#34;/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/games:/usr/local/games\u0026#34; TZ=America/New_York USERDIR=/home/username/docker MYSQL_ROOT_PASSWORD=dbrootpassword DATABASE_NAME=dbname DATABASE_USER=dbuser DATABASE_PASSWORD=dbpassword DOMAIN=yourdomain.com Save the file by “Ctrl” + “o” and then “Enter”. Use “Ctrl” + “x” to exit the file after saving.\nAfter saving, you will need to logout and log back in or by rebooting.\nsudo reboot Preparing Directories and Permissions Issue the following commands. You can copy and paste all at once or issue one at a time. The ${USERDIR} will be pulled from the environment file we just set up.\nsudo mkdir ${USERDIR} sudo mkdir ${USERDIR}/apps sudo mkdir ${USERDIR}/wordpress sudo mkdir ${USERDIR}/wordpress/wp-data sudo mkdir ${USERDIR}/data sudo mkdir ${USERDIR}/data/configurations sudo touch ${USERDIR}/data/acme.json sudo chmod 600 ${USERDIR}/data/acme.json Create Traefik static configuration file. Issue this command to create the Traefik static configuration file. ${USERDIR} will be pulled from the environment file.\nsudo nano ${USERDIR}/data/traefik.yml Copy the following into the file. Be sure to change the email address in red to your own email address.\nNote that there are two caServer addresses at the bottom of the file. The one with “staging” in the address is for testing. You may want to use this address until you get everything working. However, you will get security warnings from browsers while using the staging server certificates. The actual LetsEncrypt server will only allow you to get a few certificates over a time period before it locks you out. Put a “#” in front of the caServer address that you do not want to use. Ubuntu ignores lines with “#” in front of them.\napi: dashboard: true entryPoints: http: address: \u0026#34;:80\u0026#34; http: redirections: entryPoint: to: https https: address: \u0026#34;:443\u0026#34; http: middlewares: - secureHeaders@file - page-ratelimit@file tls: certResolver: letsencrypt providers: docker: endpoint: \u0026#34;unix:///var/run/docker.sock\u0026#34; exposedByDefault: false file: filename: /configurations/dynamic.yml certificatesResolvers: letsencrypt: acme: email: you@yourdomain.com storage: acme.json keyType: EC384 httpChallenge: entryPoint: http #caServer: https://acme-staging-v02.api.letsencrypt.org/directory caServer: https://acme-v02.api.letsencrypt.org/directory Once you have copied the text above into the file, save the file with “Ctrl” + “o” and then “Enter”. Exit the file with “Ctrl” + “x”.\nPrepare Traefik and Create Traefik Dynamic Config The Traefik password must be hashed using MD5, SHA1, or BCrypt. I recommend using BCrypt since it is more secure.\nYou can use an online Htpassword generator to hash your password. For example, if your username is “admin” and password is “password1234”, the BCrypt hashed version will look something like this: “admin:$2y$10$d0yk7WE.XqhF5bT1DdJhduRFOM5JSabTiSFCTnbC2.JgMolypHgS2”. I strongly recommend using a password manager (I use 1Password) to generate a high bit-count password to use in this step.\nOnce you have created your username, password, and hashed version of the password, continue to the next steps.\nIssue this command to create the Traefik dynamic configuration file. ${USERDIR} will be pulled from the environment file.\nsudo nano ${USERDIR}/data/configurations/dynamic.yml Copy the following into the dynamic.yml file. Update the username and password (in red) with the hashed values you just created above.\nNote that this file uses the page-ratelimit middleware. The values below allow 50 requests per second average with a burst of 50. You can change this if desired.\nhttp: middlewares: secureHeaders: headers: frameDeny: true sslRedirect: true browserXssFilter: true contentTypeNosniff: true forceSTSHeader: true stsIncludeSubdomains: true stsPreload: true stsSeconds: 31536000 customFrameOptionsValue: SAMEORIGIN user-auth: basicAuth: users: - \u0026#34;admin:$2y$10$d0yk7WE.XqhF5bT1DdJhduRFOM5JSabTiSFCTnbC2.JgMolypHgS2\u0026#34; page-ratelimit: rateLimit: average: 50 burst: 50 tls: options: default: cipherSuites: - TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384 - TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256 - TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305 - TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305 minVersion: VersionTLS12 Once you have copied the text above into the file, save the file with “Ctrl” + “o” and then “Enter”. Exit the file with “Ctrl” + “x”.\nCreate the Traefik Docker-Compose File Issue this command to create the Traefik docker-compose file. ${USERDIR} will be pulled from the environment file.\nsudo nano ${USERDIR}/docker-compose.yml Copy the following into the file. Note that fields with “${ }” will be pulled from the environment file.\nThis file will pull the latest version of traefik (currently v2.2.6). You could limit the image to the latest version in the 2.2.X series by using image tag “traefik:chevrotin”. Note that if you use the “latest” tag, and Treafik releases the next version (v3, etc), your current configuration may not work with the new version. I prefer to use the latest tag and fix the configuration as needed. See dockerhub for the release tags.\nThis file will route ports 80 (http) and 443 (https) to the Traefik container.\nYou can also find a copy of the docker-compose.yml here on GitHub.\nversion: \u0026#39;3.3\u0026#39; services: traefik: image: traefik:latest container_name: traefik restart: always security_opt: - no-new-privileges:true ports: - 80:80 - 443:443 volumes: - /var/run/docker.sock:/var/run/docker.sock:ro - ${USERDIR}/data/traefik.yml:/traefik.yml:ro - ${USERDIR}/data/configurations:/configurations - ${USERDIR}/data/acme.json:/acme.json environment: TZ: ${TZ} networks: - proxy labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.rule=Host(`traefik.${DOMAIN}`)\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.service=api@internal\u0026#34; - \u0026#34;traefik.http.routers.traefik-secure.middlewares=user-auth@file\u0026#34; networks: proxy: external: true Once you have copied the text above into the file, save the file with “Ctrl” + “o” and then “Enter”. Exit the file with “Ctrl” + “x”.\nCreate the WordPress Docker-Compose File Issue this command to create the WordPress docker-compose file. ${USERDIR} will be pulled from the environment file.\nsudo nano ${USERDIR}/apps/docker-compose.yml Copy the following into the file. Note that fields with “${ }” will be pulled from the environment file.\nYou can also find a copy of the docker-compose.yml on GitHub:\nversion: \u0026#39;3.7\u0026#39; services: db: image: mariadb:latest container_name: wp-db volumes: - db-data:/var/lib/mysql networks: - default security_opt: - no-new-privileges:true restart: unless-stopped environment: MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD} MYSQL_DATABASE: ${DATABASE_NAME} MYSQL_USER: ${DATABASE_USER} MYSQL_PASSWORD: ${DATABASE_PASSWORD} TZ: ${TZ} wordpress: depends_on: - db image: wordpress:latest container_name: wordpress environment: WORDPRESS_DB_HOST: db:3306 WORDPRESS_DB_NAME: ${DATABASE_NAME} WORDPRESS_DB_USER: ${DATABASE_USER} WORDPRESS_DB_PASSWORD: ${DATABASE_PASSWORD} TZ: ${TZ} volumes: - ${USERDIR}/wordpress/wp-data:/var/www/html networks: - proxy - default security_opt: - no-new-privileges:true restart: unless-stopped labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.wordpress-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.wordpress-secure.rule=Host(`${DOMAIN}`)\u0026#34; portainer: container_name: portainer image: portainer/portainer:latest restart: unless-stopped command: -H unix:///var/run/docker.sock networks: - proxy - default security_opt: - no-new-privileges:true volumes: - /var/run/docker.sock:/var/run/docker.sock:ro environment: TZ: ${TZ} labels: - \u0026#34;traefik.enable=true\u0026#34; - \u0026#34;traefik.docker.network=proxy\u0026#34; - \u0026#34;traefik.http.routers.portainer-secure.entrypoints=https\u0026#34; - \u0026#34;traefik.http.routers.portainer-secure.rule=Host(`portainer.${DOMAIN}`)\u0026#34; volumes: db-data: networks: proxy: external: true Create the Proxy Network and Start Traefik Now that the files are created, you are ready to create the proxy network and start Traefik.\nIssue this command to create the proxy network:\ndocker network create proxy Issue this command to start the Traefik container. ${USERDIR} will be pulled from the environment file.\ndocker-compose -f ${USERDIR}/docker-compose.yml up -d Once that is complete, wait a few minutes and attempt to access the Traefik web admin page. You should be able to reach it at https://traefik.yourdomain.com. Replace “yourdomain.com” with your actual domain matching the domain you added to the environment file. If it works, use the non-hashed username and password. You should see this:\nIf you are unable to access this page, you will likely need to forward ports 80 and 443 to your Ubuntu server (see the below screenshot for an example of the ports forwarded to Ubuntu(10.100.0.15), and also see this post for recommendations on how to secure your network). If you are using a VPS or other externally hosted option, this will not be necessary. Once your ports are forwarded, you should be ready to continue.\nStart the Remaining Containers Warning – It is important that you login to WordPress and Portainer after you start them so that you can set passwords. If you do not, anyone could have access to them!\nIssue this command to start the remaining containers. ${USERDIR} will be pulled from the environment file.\ndocker-compose -f ${USERDIR}/apps/docker-compose.yml up -d Give it a few minutes and attempt to access WordPress and Portainer from a browser.\n WordPress:\n You should be able to access WordPress at “https://yourdomain.com“. Replace “yourdomain.com” with your domain.\nIf successful, you should see the initial WordPress screen where you create your WordPress user and password. Use a really strong password for WordPress (again, I recommend using a password manager like 1Password).\nIf the WordPress installation was successful, you should see a message declaring success.\n Portainer:\n You should be able to access Portainer at “https://portainer.yourdomain.com“. Replace “yourdomain.com” with your domain created in section 1.\nIf successful, you should see the initial Portainer screen where you create your Portainer user and password. Use a really strong password for Portainer (again, I recommend using a password manager like 1Password).\nAfter you create your Portainer password, login to Portainer, and click “containers” on the left side of the page. You will see a list of your running containers. You can stop, start, restart, remove, etc. installed containers.\nGetting Started with WordPress A solid recommended thing to do with a new WordPress site is increase the php memory limit and max file upload size. One way to do this is to modify the .htaccess file. To change this file, do the following in terminal. ${USERDIR} will be pulled from the environment file. The file path below assumes you specified the WordPress volume as shown in the docker-compose file.\nsudo nano ${USERDIR}/wordpress/wp-data/.htaccess Add these lines to the file:\nphp_value memory_limit 256M php_value upload_max_filesize 128M php_value post_max_size 128M php_value max_execution_time 300 php_value max_input_time 1000 Once you have copied the text above into the file, save the file with “Ctrl” + “o” and then “Enter”. Exit the file with “Ctrl” + “x”.\nRestart the WordPress container for changes to take effect. You can restart it from Portainer or restart all containers with this command:\ndocker restart $(docker ps -q)  WordPress Admin Page:\n To reach your WordPress admin page, open a browser and navigate to “https://yourdomain.com/wp-admin” (replace “yourdomain.com” with your actual domain). Log in using the credentials you created in Section 14.\n Change the WordPress Theme:\n You can change the theme of your blog or website if you want.\nAfter clicking “Add New Theme”, search your favorite theme and put your mouse over it and click “Install”. After it installs, click “Activate”.\n Create a Blog Post:\n To add a blog post, click “Posts” and then “Add New”. A new, blank blog post will be created. I like using the default block editor that comes with WordPress. You can add page builder plugins if you want. This is what the default block editor looks like:\nAfter creating your post using the block editor, click “Publish” and then “Publish” again to make the post available. Navigate to the address shown on the “Post address”. Your blog post will look something like this. You can customize your site by changing settings or adding plug-ins.\n","permalink":"https://whitematter.tech/posts/hosting-your-own-site-with-traefik-and-wordpress/","summary":"My first post will, appropriately, show you how to build your own self-hosted Wordpress site utilizing Docker (just like this site)! For this setup, I am using a Ubuntu bare-metal machine behind a Unifi Dream Machine Pro . You can use a VPS or an OS on bare-metal capable of running Docker (for this tutorial though, we will use tools only applicable to Ubuntu, but you can make adjustments where necessary if you are familiar with Docker and choose not to use Ubuntu).","title":"Host Your Own Free Wordpress Site with Traefik and Docker"}]